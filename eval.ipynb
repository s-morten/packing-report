{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soccerdata as sd\n",
    "from pathlib import PosixPath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import cProfile\n",
    "from tqdm import tqdm\n",
    "import database_io.dims\n",
    "import database_io.faks\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from sqlalchemy import func\n",
    "from sqlalchemy.orm import aliased\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"/home/morten/Develop/packing-report/gde/GDE.db\"\n",
    "\n",
    "engine = create_engine(f'sqlite:///{db_path}', echo=False)\n",
    "session = Session(engine)\n",
    "\n",
    "\n",
    "# Joining the subqueries\n",
    "query = (\n",
    "    session.query(\n",
    "        database_io.dims.Games.expected_game_result,\n",
    "        # database_io.dims.Games.result\n",
    "    ).filter(database_io.dims.Games.minutes > 80)\n",
    "    .filter(database_io.dims.Games.expected_game_result >= 0)\n",
    "    .filter(database_io.dims.Games.version == 0.2)\n",
    ")\n",
    "\n",
    "results = query.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(pd.DataFrame(results, columns=['expected_game_result']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data. Train + Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"/home/morten/Develop/packing-report/gde/GDE.db\"\n",
    "\n",
    "engine = create_engine(f'sqlite:///{db_path}', echo=False)\n",
    "session = Session(engine)\n",
    "\n",
    "\n",
    "# Subquery for result_table\n",
    "ranked_subquery = (\n",
    "    session.query(\n",
    "        database_io.dims.Games.game_id,\n",
    "        database_io.dims.Games.team_id,\n",
    "        database_io.dims.Games.result,\n",
    "        func.row_number().over(\n",
    "            partition_by=(database_io.dims.Games.game_id, database_io.dims.Games.team_id),\n",
    "            order_by=func.count().desc()\n",
    "        ).label('rank')\n",
    "    )\n",
    "    .filter(database_io.dims.Games.game_date > '2018-06-01')\n",
    "    .group_by(database_io.dims.Games.game_id, database_io.dims.Games.team_id, database_io.dims.Games.result)\n",
    "    .subquery()\n",
    ")\n",
    "\n",
    "result_table = (\n",
    "    session.query(ranked_subquery.c.game_id, ranked_subquery.c.team_id, ranked_subquery.c.result)\n",
    "    .filter(ranked_subquery.c.rank == 1)\n",
    "    .subquery()\n",
    ")\n",
    "\n",
    "# Subquery for elo_table\n",
    "elo_table = (\n",
    "    session.query(\n",
    "        func.avg(database_io.dims.Games.elo).label('avg_elo'),\n",
    "        database_io.dims.Games.team_id,\n",
    "        database_io.dims.Games.game_id,\n",
    "        database_io.dims.Games.game_date,\n",
    "        database_io.dims.Games.home\n",
    "    )\n",
    "    .filter(database_io.dims.Games.game_date > '2018-06-01')\n",
    "    .group_by(database_io.dims.Games.game_id, database_io.dims.Games.team_id)\n",
    "    .subquery()\n",
    ")\n",
    "\n",
    "# Joining the subqueries\n",
    "query = (\n",
    "    session.query(\n",
    "        result_table.c.game_id,\n",
    "        result_table.c.team_id,\n",
    "        result_table.c.result,\n",
    "        elo_table.c.avg_elo,\n",
    "        elo_table.c.game_date, \n",
    "        elo_table.c.home\n",
    "\n",
    "    )\n",
    "    .join(elo_table, \n",
    "          (result_table.c.game_id == elo_table.c.game_id) & \n",
    "          (result_table.c.team_id == elo_table.c.team_id))\n",
    ")\n",
    "\n",
    "results = query.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(np.array(results), columns=[\"game_id\", \"team_id\", \"result\", \"avg_elo\", \"date\", \"home\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[data[\"date\"] <= \"2021-07-01\"].copy()\n",
    "test_data = data[data[\"date\"] > \"2021-07-01\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.merge(train_data, train_data, how=\"outer\", on=\"game_id\")\n",
    "train_data = train_data.loc[train_data[\"team_id_x\"] != train_data[\"team_id_y\"]]\n",
    "\n",
    "test_data = pd.merge(test_data, test_data, how=\"outer\", on=\"game_id\")\n",
    "test_data = test_data.loc[test_data[\"team_id_x\"] != test_data[\"team_id_y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[train_data[\"home_x\"] == \"1\"]\n",
    "test_data = test_data[test_data[\"home_x\"] == \"1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOV Regressor analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"goals_x\"] = train_data[\"result_x\"].apply(lambda x: int(x.split(\"-\")[0]))\n",
    "train_data[\"goals_y\"] = train_data[\"result_y\"].apply(lambda x: int(x.split(\"-\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data.columns] = train_data[train_data.columns].apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "\n",
    "with pm.Model() as game_model:\n",
    "    home_advantage = pm.Normal(\"home_advantage\", 0, 10)\n",
    "    home_elo_diff = pm.Normal(\"home_elo\", 0, 10) \n",
    "    away_elo_diff = pm.Normal(\"away_elo\", 0, 10)\n",
    "\n",
    "    # theta_h = home_advantage + home_elo_diff * (train_data[\"avg_elo_x\"] - train_data[\"avg_elo_y\"])\n",
    "    theta_h = home_advantage + home_elo_diff * (train_data[\"avg_elo_x\"] - train_data[\"avg_elo_y\"])\n",
    "    theta_a = away_elo_diff * (train_data[\"avg_elo_y\"] - train_data[\"avg_elo_x\"])\n",
    "\n",
    "    goals_h = pm.Poisson(\"goals_h\", pm.math.exp(theta_h), observed=train_data[\"goals_x\"])\n",
    "    goals_a = pm.Poisson(\"goals_a\", pm.math.exp(theta_a), observed=train_data[\"goals_y\"])\n",
    "\n",
    "    # # observed\n",
    "    # goal_diff = goals_h - goals_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with game_model:\n",
    "    trace = pm.sample(tune=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[test_data.columns] = test_data[test_data.columns].apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"win_x\"] = test_data.apply(lambda x: np.count_nonzero(np.random.poisson(np.exp(trace.posterior.home_advantage[0] + trace.posterior.home_elo[0] * (x[\"avg_elo_x\"] - x[\"avg_elo_y\"]))) - \n",
    "                                               np.random.poisson(np.exp(trace.posterior.away_elo[0] * (x[\"avg_elo_y\"] - x[\"avg_elo_x\"]))) > 0) / 1000, \n",
    "                                               axis=1)\n",
    "test_data[\"draw\"] = test_data.apply(lambda x: np.count_nonzero(np.random.poisson(np.exp(trace.posterior.home_advantage[0] + trace.posterior.home_elo[0] * (x[\"avg_elo_x\"] - x[\"avg_elo_y\"]))) - \n",
    "                                               np.random.poisson(np.exp(trace.posterior.away_elo[0] * (x[\"avg_elo_y\"] - x[\"avg_elo_x\"]))) == 0) / 1000, \n",
    "                                               axis=1)\n",
    "test_data[\"win_y\"] = test_data.apply(lambda x: np.count_nonzero(np.random.poisson(np.exp(trace.posterior.home_advantage[0] + trace.posterior.home_elo[0] * (x[\"avg_elo_x\"] - x[\"avg_elo_y\"]))) - \n",
    "                                               np.random.poisson(np.exp(trace.posterior.away_elo[0] * (x[\"avg_elo_y\"] - x[\"avg_elo_x\"]))) < 0) / 1000, \n",
    "                                               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"sum\"] = test_data[\"win_x\"] + test_data[\"draw\"] + test_data[\"win_y\"] # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_multi(targets, probs):\n",
    "    return np.mean(np.sum((probs - targets)**2, axis=1))\n",
    "\n",
    "labels = [[1, 0, 0] if int(x.split(\"-\")[0]) > int(x.split(\"-\")[1]) else [0, 1, 0] if int(x.split(\"-\")[0]) == int(x.split(\"-\")[1]) else [0, 0, 1] for x in test_data.result_x]\n",
    "probs = [[x, y, z] for x, y, z in zip(test_data.win_x, test_data.draw, test_data.win_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brier_multi(np.array(labels), np.array(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "packing-report",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
