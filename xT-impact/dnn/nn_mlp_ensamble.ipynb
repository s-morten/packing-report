{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 10:00:36.044245: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 10:00:36.203727: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-24 10:00:36.203749: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-24 10:00:37.082868: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-24 10:00:37.082948: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-24 10:00:37.082958: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from custum_func import my_acc, my_loss, ExponentialLayer, ControlledDropoutLayer, dropout_conf_1, dropout_conf_2, dropout_conf_3, PowLayer, dropout_conf_4\n",
    "import itertools\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, InputLayer\n",
    "from keras import initializers, callbacks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train_data\n",
    "\n",
    "train_data = pd.read_csv(\n",
    "    \"../data_acquisition/data_0.3/training_standardized.csv\", sep=\";\"\n",
    ")\n",
    "\n",
    "features = np.swapaxes(\n",
    "    np.array(\n",
    "        [\n",
    "            train_data[\"home_xG\"],\n",
    "            train_data[\"home_xg_against\"],\n",
    "            train_data[\"away_xG\"],\n",
    "            train_data[\"away_xg_against\"],\n",
    "            train_data[\"home_xT_all\"],\n",
    "            train_data[\"home_xt_all_against\"],\n",
    "            train_data[\"away_xT_all\"],\n",
    "            train_data[\"away_xt_all_against\"],\n",
    "        ]\n",
    "    ),\n",
    "    0,\n",
    "    1,\n",
    ")\n",
    "\n",
    "form = np.swapaxes(\n",
    "    np.array(\n",
    "        [\n",
    "            (train_data[\"ha_form_home_for\"]),\n",
    "            (train_data[\"ha_form_home_against\"]),\n",
    "            (train_data[\"ha_form_away_for\"]),\n",
    "            (train_data[\"ha_form_away_against\"]),\n",
    "        ]\n",
    "    ),\n",
    "    0,\n",
    "    1,\n",
    ")\n",
    "\n",
    "goals = np.swapaxes(\n",
    "    np.array([train_data[\"home_score\"], train_data[\"away_score\"]]), 0, 1\n",
    ")\n",
    "\n",
    "elo_diff = np.swapaxes(\n",
    "    np.array([(train_data[\"elo_diff_home\"]), (train_data[\"elo_diff_away\"])]), 0, 1\n",
    ")\n",
    "\n",
    "X = np.concatenate([features, form, elo_diff], axis=1)\n",
    "y = np.array(goals, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\n",
    "    \"../data_acquisition/data_0.3/test_standardized.csv\", sep=\";\"\n",
    ").dropna()\n",
    "\n",
    "\n",
    "features = np.swapaxes(\n",
    "    np.array(\n",
    "        [\n",
    "            test_data[\"home_xG\"],\n",
    "            test_data[\"home_xg_against\"],\n",
    "            test_data[\"away_xG\"],\n",
    "            test_data[\"away_xg_against\"],\n",
    "            test_data[\"home_xT_all\"],\n",
    "            test_data[\"home_xt_all_against\"],\n",
    "            test_data[\"away_xT_all\"],\n",
    "            test_data[\"away_xt_all_against\"],\n",
    "        ]\n",
    "    ),\n",
    "    0,\n",
    "    1,\n",
    ")\n",
    "\n",
    "form = np.swapaxes(\n",
    "    np.array(\n",
    "        [\n",
    "            (test_data[\"ha_form_home_for\"]),\n",
    "            (test_data[\"ha_form_home_against\"]),\n",
    "            (test_data[\"ha_form_away_for\"]),\n",
    "            (test_data[\"ha_form_away_against\"]),\n",
    "        ]\n",
    "    ),\n",
    "    0,\n",
    "    1,\n",
    ")\n",
    "\n",
    "goals = np.swapaxes(np.array([test_data[\"home_score\"], test_data[\"away_score\"]]), 0, 1)\n",
    "\n",
    "elo = np.swapaxes(\n",
    "    np.array([(test_data[\"elo_diff_home\"]), (test_data[\"elo_diff_away\"])]), 0, 1\n",
    ")\n",
    "X_test = np.concatenate([features, form, elo], axis=1)\n",
    "y_test = np.array(goals, dtype=float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 10:00:38.116191: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-24 10:00:38.116322: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-24 10:00:38.116381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (morten-thinkpad): /proc/driver/nvidia/version does not exist\n",
      "2023-04-24 10:00:38.117148: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 1s 1ms/step - loss: 226.3370 - my_acc: 0.4590\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 155.8254 - my_acc: 0.4631\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 111.9897 - my_acc: 0.4471\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 83.9011 - my_acc: 0.4418\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 65.3481 - my_acc: 0.4045\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 52.3759 - my_acc: 0.4078\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 43.1866 - my_acc: 0.4082\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 36.2324 - my_acc: 0.4070\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 30.9987 - my_acc: 0.4152\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 26.8778 - my_acc: 0.4164\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 23.6084 - my_acc: 0.4254\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 21.0549 - my_acc: 0.4107\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 18.9891 - my_acc: 0.4160\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 17.3541 - my_acc: 0.4230\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 15.9702 - my_acc: 0.4275\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 14.7708 - my_acc: 0.4221\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 13.7608 - my_acc: 0.4266\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 12.8778 - my_acc: 0.4328\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 12.1116 - my_acc: 0.4279\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 11.4388 - my_acc: 0.4385\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 10.8118 - my_acc: 0.4434\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 10.2551 - my_acc: 0.4422\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 9.7504 - my_acc: 0.4451\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 9.2743 - my_acc: 0.4414\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.8584 - my_acc: 0.4525\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.4525 - my_acc: 0.4492\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.0852 - my_acc: 0.4541\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.7705 - my_acc: 0.4566\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.4273 - my_acc: 0.4602\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.1417 - my_acc: 0.4652\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.8525 - my_acc: 0.4643\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.5878 - my_acc: 0.4734\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.3456 - my_acc: 0.4725\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.1141 - my_acc: 0.4713\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.8925 - my_acc: 0.4807\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.6942 - my_acc: 0.4762\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.5000 - my_acc: 0.4865\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.3182 - my_acc: 0.4795\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.1426 - my_acc: 0.4906\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.9807 - my_acc: 0.4865\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.8219 - my_acc: 0.4926\n",
      "Epoch 42/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 4.6722 - my_acc: 0.4824\n",
      "Epoch 43/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.5297 - my_acc: 0.4852\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.4022 - my_acc: 0.4803\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.2774 - my_acc: 0.4914\n",
      "Epoch 46/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.1463 - my_acc: 0.4930\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.0289 - my_acc: 0.4910\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.9238 - my_acc: 0.4918\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.8208 - my_acc: 0.4922\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.7160 - my_acc: 0.4865\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.6247 - my_acc: 0.5000\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.5305 - my_acc: 0.4898\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.4497 - my_acc: 0.4918\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.3648 - my_acc: 0.4898\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.2884 - my_acc: 0.4861\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.2057 - my_acc: 0.4959\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.1369 - my_acc: 0.4959\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.0579 - my_acc: 0.4955\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.9851 - my_acc: 0.4943\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.9247 - my_acc: 0.5033\n",
      "Epoch 61/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.8663 - my_acc: 0.4865\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.8024 - my_acc: 0.4967\n",
      "Epoch 63/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.7447 - my_acc: 0.4914\n",
      "Epoch 64/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.6936 - my_acc: 0.4980\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.6331 - my_acc: 0.4943\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.5857 - my_acc: 0.4984\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.5412 - my_acc: 0.5025\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.4867 - my_acc: 0.5061\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.4408 - my_acc: 0.4963\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.3967 - my_acc: 0.4943\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.3535 - my_acc: 0.5033\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.3121 - my_acc: 0.5004\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2788 - my_acc: 0.5053\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2285 - my_acc: 0.5033\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2070 - my_acc: 0.5070\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1614 - my_acc: 0.5033\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1257 - my_acc: 0.5098\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0964 - my_acc: 0.5131\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0707 - my_acc: 0.5045\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0442 - my_acc: 0.5045\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0116 - my_acc: 0.5242\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9826 - my_acc: 0.5053\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9537 - my_acc: 0.5123\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9297 - my_acc: 0.5115\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9049 - my_acc: 0.5279\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8788 - my_acc: 0.5180\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8590 - my_acc: 0.5184\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8263 - my_acc: 0.5209\n",
      "Epoch 89/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8053 - my_acc: 0.5352\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7807 - my_acc: 0.5369\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7660 - my_acc: 0.5316\n",
      "Epoch 92/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7477 - my_acc: 0.5266\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7303 - my_acc: 0.5406\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7073 - my_acc: 0.5361\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6897 - my_acc: 0.5406\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6750 - my_acc: 0.5414\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6571 - my_acc: 0.5410\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6386 - my_acc: 0.5451\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6193 - my_acc: 0.5426\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6054 - my_acc: 0.5393\n",
      "Epoch 1/100\n",
      "61/61 [==============================] - 1s 3ms/step - loss: 103.7495 - my_acc: 0.4689\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 69.0074 - my_acc: 0.4656\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 48.6146 - my_acc: 0.4520\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 36.8391 - my_acc: 0.4475\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 29.5655 - my_acc: 0.4373\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 24.9045 - my_acc: 0.4221\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 21.5964 - my_acc: 0.3951\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 19.0947 - my_acc: 0.3836\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 17.0238 - my_acc: 0.3971\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 15.2677 - my_acc: 0.3934\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 13.7616 - my_acc: 0.4033\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 12.4708 - my_acc: 0.4082\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 11.3358 - my_acc: 0.4094\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 10.3199 - my_acc: 0.4205\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 9.4363 - my_acc: 0.4258\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.6916 - my_acc: 0.4373\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.0411 - my_acc: 0.4410\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.4345 - my_acc: 0.4467\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.9275 - my_acc: 0.4504\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.4630 - my_acc: 0.4525\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.0618 - my_acc: 0.4611\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.7075 - my_acc: 0.4742\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.3955 - my_acc: 0.4750\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.1086 - my_acc: 0.4939\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.8595 - my_acc: 0.4988\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.6335 - my_acc: 0.4996\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.4258 - my_acc: 0.4996\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.2431 - my_acc: 0.5098\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 4.0752 - my_acc: 0.5135\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.9181 - my_acc: 0.5135\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.7797 - my_acc: 0.5303\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.6463 - my_acc: 0.5316\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.5300 - my_acc: 0.5344\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.4226 - my_acc: 0.5307\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.3231 - my_acc: 0.5254\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.2274 - my_acc: 0.5414\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.1313 - my_acc: 0.5402\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.0516 - my_acc: 0.5439\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.9738 - my_acc: 0.5414\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.9000 - my_acc: 0.5389\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.8330 - my_acc: 0.5414\n",
      "Epoch 42/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.7712 - my_acc: 0.5398\n",
      "Epoch 43/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.7053 - my_acc: 0.5439\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.6437 - my_acc: 0.5426\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.5912 - my_acc: 0.5451\n",
      "Epoch 46/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.5324 - my_acc: 0.5566\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.4828 - my_acc: 0.5561\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.4368 - my_acc: 0.5533\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.3930 - my_acc: 0.5557\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.3510 - my_acc: 0.5607\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2980 - my_acc: 0.5709\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2608 - my_acc: 0.5730\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2230 - my_acc: 0.5750\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1837 - my_acc: 0.5730\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1516 - my_acc: 0.5926\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1160 - my_acc: 0.5787\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0836 - my_acc: 0.5893\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0534 - my_acc: 0.5869\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0263 - my_acc: 0.5906\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9933 - my_acc: 0.5922\n",
      "Epoch 61/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9603 - my_acc: 0.5955\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9358 - my_acc: 0.5926\n",
      "Epoch 63/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9171 - my_acc: 0.5980\n",
      "Epoch 64/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8846 - my_acc: 0.6012\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8636 - my_acc: 0.6037\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8426 - my_acc: 0.5939\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8138 - my_acc: 0.6086\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8010 - my_acc: 0.5996\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7709 - my_acc: 0.5984\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7529 - my_acc: 0.6041\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7358 - my_acc: 0.6115\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7132 - my_acc: 0.6176\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6960 - my_acc: 0.6143\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6773 - my_acc: 0.6123\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6656 - my_acc: 0.6111\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6428 - my_acc: 0.6119\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6347 - my_acc: 0.6148\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6174 - my_acc: 0.6119\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.5999 - my_acc: 0.6139\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5826 - my_acc: 0.6139\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5749 - my_acc: 0.6217\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5637 - my_acc: 0.6201\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5470 - my_acc: 0.6086\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5361 - my_acc: 0.6156\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5331 - my_acc: 0.6152\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5105 - my_acc: 0.6152\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5000 - my_acc: 0.6102\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4901 - my_acc: 0.6152\n",
      "Epoch 89/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4840 - my_acc: 0.6119\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4670 - my_acc: 0.6176\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4608 - my_acc: 0.6135\n",
      "Epoch 92/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4457 - my_acc: 0.6164\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4412 - my_acc: 0.6078\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4222 - my_acc: 0.6160\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4134 - my_acc: 0.6139\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4116 - my_acc: 0.6221\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3939 - my_acc: 0.6189\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3909 - my_acc: 0.6234\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3824 - my_acc: 0.6234\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3728 - my_acc: 0.6291\n",
      "Epoch 1/100\n",
      "61/61 [==============================] - 1s 2ms/step - loss: 65.5053 - my_acc: 0.4148\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 46.3077 - my_acc: 0.4131\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 34.0928 - my_acc: 0.4053\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 26.2001 - my_acc: 0.3898\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 20.8397 - my_acc: 0.4066\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 17.0494 - my_acc: 0.3934\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 14.3128 - my_acc: 0.3914\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 12.1978 - my_acc: 0.4016\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 10.5860 - my_acc: 0.4107\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 9.3302 - my_acc: 0.4176\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.3253 - my_acc: 0.4225\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.5039 - my_acc: 0.4230\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.8374 - my_acc: 0.4234\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.2827 - my_acc: 0.4266\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.8223 - my_acc: 0.4291\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.4201 - my_acc: 0.4246\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.0841 - my_acc: 0.4283\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.7764 - my_acc: 0.4221\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.5106 - my_acc: 0.4242\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.2788 - my_acc: 0.4340\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.0742 - my_acc: 0.4262\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.8897 - my_acc: 0.4377\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.7182 - my_acc: 0.4389\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.5603 - my_acc: 0.4377\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.4220 - my_acc: 0.4410\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.2962 - my_acc: 0.4406\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.1796 - my_acc: 0.4484\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.0703 - my_acc: 0.4508\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.9734 - my_acc: 0.4520\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.8843 - my_acc: 0.4561\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.7950 - my_acc: 0.4561\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.7219 - my_acc: 0.4648\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.6400 - my_acc: 0.4619\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.5708 - my_acc: 0.4738\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.5061 - my_acc: 0.4717\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.4377 - my_acc: 0.4762\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.3803 - my_acc: 0.4795\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.3286 - my_acc: 0.4963\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2709 - my_acc: 0.4967\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2228 - my_acc: 0.4996\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1759 - my_acc: 0.4955\n",
      "Epoch 42/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1269 - my_acc: 0.5029\n",
      "Epoch 43/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0804 - my_acc: 0.5057\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0367 - my_acc: 0.5217\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9980 - my_acc: 0.5213\n",
      "Epoch 46/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9609 - my_acc: 0.5299\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9237 - my_acc: 0.5266\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8885 - my_acc: 0.5344\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8589 - my_acc: 0.5377\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8268 - my_acc: 0.5262\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7943 - my_acc: 0.5348\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7658 - my_acc: 0.5291\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7380 - my_acc: 0.5332\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7077 - my_acc: 0.5418\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6833 - my_acc: 0.5434\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6602 - my_acc: 0.5316\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6337 - my_acc: 0.5303\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6126 - my_acc: 0.5357\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5938 - my_acc: 0.5324\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5698 - my_acc: 0.5332\n",
      "Epoch 61/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5471 - my_acc: 0.5328\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5304 - my_acc: 0.5316\n",
      "Epoch 63/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5106 - my_acc: 0.5418\n",
      "Epoch 64/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4921 - my_acc: 0.5377\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4756 - my_acc: 0.5520\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4633 - my_acc: 0.5344\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4463 - my_acc: 0.5496\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4283 - my_acc: 0.5434\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4171 - my_acc: 0.5443\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4002 - my_acc: 0.5459\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3885 - my_acc: 0.5500\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3731 - my_acc: 0.5480\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3650 - my_acc: 0.5516\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3522 - my_acc: 0.5508\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3351 - my_acc: 0.5492\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3299 - my_acc: 0.5520\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3145 - my_acc: 0.5520\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3045 - my_acc: 0.5525\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.2960 - my_acc: 0.5541\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.2858 - my_acc: 0.5520\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.2767 - my_acc: 0.5500\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.2652 - my_acc: 0.5537\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.2573 - my_acc: 0.5623\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.2528 - my_acc: 0.5693\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.2393 - my_acc: 0.5631\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.2301 - my_acc: 0.5578\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.2253 - my_acc: 0.5451\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.2160 - my_acc: 0.5557\n",
      "Epoch 89/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.2094 - my_acc: 0.5594\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.2030 - my_acc: 0.5537\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.1976 - my_acc: 0.5582\n",
      "Epoch 92/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.1900 - my_acc: 0.5709\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.1809 - my_acc: 0.5721\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.1781 - my_acc: 0.5643\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.1726 - my_acc: 0.5586\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.1641 - my_acc: 0.5602\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.1628 - my_acc: 0.5697\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.1573 - my_acc: 0.5668\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.1516 - my_acc: 0.5578\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.1484 - my_acc: 0.5631\n",
      "Epoch 1/100\n",
      "61/61 [==============================] - 1s 2ms/step - loss: 62.0954 - my_acc: 0.4000\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 49.1960 - my_acc: 0.3918\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 40.1834 - my_acc: 0.4107\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 33.6263 - my_acc: 0.4156\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 28.6871 - my_acc: 0.4189\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 24.8771 - my_acc: 0.4213\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 21.8407 - my_acc: 0.4197\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 19.4021 - my_acc: 0.4197\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 17.4701 - my_acc: 0.4447\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 15.7392 - my_acc: 0.4357\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 14.3420 - my_acc: 0.4365\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 13.1359 - my_acc: 0.4275\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 12.0781 - my_acc: 0.4311\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 11.1620 - my_acc: 0.4328\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 10.3767 - my_acc: 0.4344\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 9.7022 - my_acc: 0.4221\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 9.0784 - my_acc: 0.4275\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.5185 - my_acc: 0.4291\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.0256 - my_acc: 0.4213\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.5853 - my_acc: 0.4246\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.1806 - my_acc: 0.4250\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.8172 - my_acc: 0.4205\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.4829 - my_acc: 0.4230\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.1812 - my_acc: 0.4250\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.9050 - my_acc: 0.4324\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.6579 - my_acc: 0.4275\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.4339 - my_acc: 0.4357\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 5.1928 - my_acc: 0.4439\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.9919 - my_acc: 0.4439\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.8026 - my_acc: 0.4500\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.6273 - my_acc: 0.4529\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.4692 - my_acc: 0.4467\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.3049 - my_acc: 0.4578\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.1618 - my_acc: 0.4598\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.0279 - my_acc: 0.4537\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.8908 - my_acc: 0.4652\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.7724 - my_acc: 0.4627\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.6625 - my_acc: 0.4791\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.5507 - my_acc: 0.4770\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.4503 - my_acc: 0.4885\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.3554 - my_acc: 0.4730\n",
      "Epoch 42/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.2585 - my_acc: 0.4893\n",
      "Epoch 43/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.1772 - my_acc: 0.4918\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.0981 - my_acc: 0.4861\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.0158 - my_acc: 0.4947\n",
      "Epoch 46/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.9409 - my_acc: 0.4963\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.8752 - my_acc: 0.4984\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.8169 - my_acc: 0.4951\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.7489 - my_acc: 0.4939\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.6937 - my_acc: 0.4861\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.6416 - my_acc: 0.5016\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.5827 - my_acc: 0.4963\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.5304 - my_acc: 0.4967\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.4828 - my_acc: 0.4939\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.4447 - my_acc: 0.5053\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.3986 - my_acc: 0.4996\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.3496 - my_acc: 0.5033\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.3116 - my_acc: 0.5066\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2765 - my_acc: 0.5016\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2377 - my_acc: 0.5049\n",
      "Epoch 61/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1966 - my_acc: 0.5090\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1644 - my_acc: 0.5139\n",
      "Epoch 63/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1298 - my_acc: 0.5045\n",
      "Epoch 64/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0977 - my_acc: 0.5115\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0633 - my_acc: 0.5078\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0370 - my_acc: 0.5107\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.0100 - my_acc: 0.5127\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9817 - my_acc: 0.5078\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9606 - my_acc: 0.5205\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9371 - my_acc: 0.5213\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9045 - my_acc: 0.5189\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8812 - my_acc: 0.5246\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8624 - my_acc: 0.5176\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8406 - my_acc: 0.5230\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8297 - my_acc: 0.5250\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.8081 - my_acc: 0.5340\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7830 - my_acc: 0.5279\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7712 - my_acc: 0.5365\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7542 - my_acc: 0.5287\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7368 - my_acc: 0.5328\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7195 - my_acc: 0.5258\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7077 - my_acc: 0.5332\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6841 - my_acc: 0.5287\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6792 - my_acc: 0.5389\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6597 - my_acc: 0.5352\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6452 - my_acc: 0.5332\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6335 - my_acc: 0.5418\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6209 - my_acc: 0.5340\n",
      "Epoch 89/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6193 - my_acc: 0.5504\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5952 - my_acc: 0.5533\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5848 - my_acc: 0.5500\n",
      "Epoch 92/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5799 - my_acc: 0.5430\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5703 - my_acc: 0.5508\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5580 - my_acc: 0.5553\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5455 - my_acc: 0.5463\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5348 - my_acc: 0.5590\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5267 - my_acc: 0.5533\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5159 - my_acc: 0.5803\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5132 - my_acc: 0.5619\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4961 - my_acc: 0.5598\n",
      "Epoch 1/100\n",
      "61/61 [==============================] - 1s 2ms/step - loss: 53.0761 - my_acc: 0.3414\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 35.1444 - my_acc: 0.3549\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 24.7553 - my_acc: 0.3803\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 18.8972 - my_acc: 0.3832\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 15.3458 - my_acc: 0.3926\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 12.9881 - my_acc: 0.3910\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 11.3021 - my_acc: 0.3930\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 10.0024 - my_acc: 0.3918\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.9898 - my_acc: 0.4000\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.1502 - my_acc: 0.4033\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.4529 - my_acc: 0.4143\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.8610 - my_acc: 0.4193\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 6.3553 - my_acc: 0.4148\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.9022 - my_acc: 0.4152\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.5255 - my_acc: 0.4291\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.1749 - my_acc: 0.4217\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.8759 - my_acc: 0.4299\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.6006 - my_acc: 0.4250\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.3538 - my_acc: 0.4287\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.1414 - my_acc: 0.4266\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.9434 - my_acc: 0.4373\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.7664 - my_acc: 0.4279\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.6028 - my_acc: 0.4324\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.4522 - my_acc: 0.4324\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.3197 - my_acc: 0.4365\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.1992 - my_acc: 0.4410\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.0816 - my_acc: 0.4459\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.9825 - my_acc: 0.4504\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.8873 - my_acc: 0.4516\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.7990 - my_acc: 0.4574\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.7233 - my_acc: 0.4504\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.6394 - my_acc: 0.4656\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.5705 - my_acc: 0.4684\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.5108 - my_acc: 0.4615\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.4481 - my_acc: 0.4717\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.3917 - my_acc: 0.4717\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.3464 - my_acc: 0.4734\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2953 - my_acc: 0.4689\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2521 - my_acc: 0.4828\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2135 - my_acc: 0.4746\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1646 - my_acc: 0.4783\n",
      "Epoch 42/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1306 - my_acc: 0.4721\n",
      "Epoch 43/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0943 - my_acc: 0.4848\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0570 - my_acc: 0.4820\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0269 - my_acc: 0.4795\n",
      "Epoch 46/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9971 - my_acc: 0.4775\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9689 - my_acc: 0.4869\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9389 - my_acc: 0.4746\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9098 - my_acc: 0.4816\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8852 - my_acc: 0.4832\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.8612 - my_acc: 0.4922\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8388 - my_acc: 0.4893\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8186 - my_acc: 0.4906\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7958 - my_acc: 0.5029\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7690 - my_acc: 0.4922\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7520 - my_acc: 0.5016\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7353 - my_acc: 0.4934\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7064 - my_acc: 0.5000\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6948 - my_acc: 0.5008\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6681 - my_acc: 0.4996\n",
      "Epoch 61/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6590 - my_acc: 0.4984\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6389 - my_acc: 0.5000\n",
      "Epoch 63/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6214 - my_acc: 0.5045\n",
      "Epoch 64/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6050 - my_acc: 0.5094\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5893 - my_acc: 0.5012\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5741 - my_acc: 0.5098\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5593 - my_acc: 0.5074\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5459 - my_acc: 0.5148\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5344 - my_acc: 0.5029\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5172 - my_acc: 0.4967\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.5087 - my_acc: 0.5020\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4958 - my_acc: 0.5082\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4841 - my_acc: 0.5143\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4732 - my_acc: 0.5135\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4619 - my_acc: 0.5025\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4496 - my_acc: 0.5049\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4416 - my_acc: 0.5090\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4302 - my_acc: 0.5090\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4216 - my_acc: 0.5102\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4150 - my_acc: 0.5119\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4056 - my_acc: 0.5102\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4001 - my_acc: 0.5201\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3834 - my_acc: 0.5184\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3790 - my_acc: 0.5090\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3700 - my_acc: 0.5201\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3627 - my_acc: 0.5197\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.3561 - my_acc: 0.5242\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3470 - my_acc: 0.5135\n",
      "Epoch 89/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3391 - my_acc: 0.5205\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3296 - my_acc: 0.5262\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3272 - my_acc: 0.5180\n",
      "Epoch 92/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3196 - my_acc: 0.5238\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3119 - my_acc: 0.5193\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.3035 - my_acc: 0.5258\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.3020 - my_acc: 0.5270\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.2933 - my_acc: 0.5143\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.2895 - my_acc: 0.5311\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.2812 - my_acc: 0.5275\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.2722 - my_acc: 0.5303\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.2728 - my_acc: 0.5311\n",
      "Epoch 1/100\n",
      "61/61 [==============================] - 1s 2ms/step - loss: 141.8005 - my_acc: 0.2898\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 95.5183 - my_acc: 0.3041\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 70.6011 - my_acc: 0.3283\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 55.5781 - my_acc: 0.3283\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 45.8627 - my_acc: 0.3377\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 38.7759 - my_acc: 0.3447\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 33.4788 - my_acc: 0.3467\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 29.2789 - my_acc: 0.3471\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 25.9438 - my_acc: 0.3611\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 23.1975 - my_acc: 0.3734\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 20.9492 - my_acc: 0.3693\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 19.0872 - my_acc: 0.3828\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 17.5399 - my_acc: 0.3713\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 16.1770 - my_acc: 0.3848\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 14.9965 - my_acc: 0.3930\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 13.9594 - my_acc: 0.3959\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 13.0728 - my_acc: 0.4029\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 12.2428 - my_acc: 0.4000\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 11.5272 - my_acc: 0.4078\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 10.8948 - my_acc: 0.4127\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 10.2918 - my_acc: 0.4053\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 9.7657 - my_acc: 0.4135\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 9.2590 - my_acc: 0.4156\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.7991 - my_acc: 0.4078\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.3657 - my_acc: 0.4057\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.9696 - my_acc: 0.4193\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.6032 - my_acc: 0.4193\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.2685 - my_acc: 0.4164\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.9290 - my_acc: 0.4279\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.6271 - my_acc: 0.4393\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.3717 - my_acc: 0.4385\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.0981 - my_acc: 0.4447\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.8537 - my_acc: 0.4467\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.6338 - my_acc: 0.4615\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.4120 - my_acc: 0.4586\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.2086 - my_acc: 0.4631\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.0164 - my_acc: 0.4717\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.8441 - my_acc: 0.4742\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.6683 - my_acc: 0.4807\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.5054 - my_acc: 0.4902\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.3485 - my_acc: 0.4816\n",
      "Epoch 42/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.2051 - my_acc: 0.4934\n",
      "Epoch 43/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.0782 - my_acc: 0.4992\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.9402 - my_acc: 0.5057\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.8269 - my_acc: 0.4963\n",
      "Epoch 46/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.7104 - my_acc: 0.5119\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.5913 - my_acc: 0.5283\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.4964 - my_acc: 0.5283\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.3916 - my_acc: 0.5291\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.2919 - my_acc: 0.5369\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.2085 - my_acc: 0.5447\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.1250 - my_acc: 0.5357\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.0432 - my_acc: 0.5418\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.9611 - my_acc: 0.5348\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.8851 - my_acc: 0.5344\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.8166 - my_acc: 0.5439\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.7468 - my_acc: 0.5398\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.6738 - my_acc: 0.5484\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.6158 - my_acc: 0.5488\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.5599 - my_acc: 0.5426\n",
      "Epoch 61/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.5004 - my_acc: 0.5492\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.4547 - my_acc: 0.5492\n",
      "Epoch 63/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.3933 - my_acc: 0.5447\n",
      "Epoch 64/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.3387 - my_acc: 0.5492\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2965 - my_acc: 0.5537\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2418 - my_acc: 0.5545\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2027 - my_acc: 0.5553\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1601 - my_acc: 0.5529\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1204 - my_acc: 0.5484\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0889 - my_acc: 0.5500\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0402 - my_acc: 0.5434\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0073 - my_acc: 0.5545\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9771 - my_acc: 0.5607\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9383 - my_acc: 0.5430\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9061 - my_acc: 0.5537\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8736 - my_acc: 0.5586\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8368 - my_acc: 0.5553\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8143 - my_acc: 0.5627\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.7874 - my_acc: 0.5721\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7607 - my_acc: 0.5656\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7369 - my_acc: 0.5709\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7120 - my_acc: 0.5795\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6921 - my_acc: 0.5791\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6689 - my_acc: 0.5672\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6531 - my_acc: 0.5635\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6334 - my_acc: 0.5754\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.6091 - my_acc: 0.5717\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5903 - my_acc: 0.5672\n",
      "Epoch 89/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5739 - my_acc: 0.5807\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5652 - my_acc: 0.5689\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5436 - my_acc: 0.5742\n",
      "Epoch 92/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5253 - my_acc: 0.5799\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5169 - my_acc: 0.5697\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4960 - my_acc: 0.5787\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.4793 - my_acc: 0.5844\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4681 - my_acc: 0.5680\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4576 - my_acc: 0.5754\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4463 - my_acc: 0.5586\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4443 - my_acc: 0.5709\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4202 - my_acc: 0.5816\n",
      "Epoch 1/100\n",
      "61/61 [==============================] - 1s 2ms/step - loss: 51.0553 - my_acc: 0.3889\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 37.6327 - my_acc: 0.4123\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 30.7497 - my_acc: 0.4348\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 25.9217 - my_acc: 0.4377\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 22.0941 - my_acc: 0.4520\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 19.1793 - my_acc: 0.4561\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 16.7999 - my_acc: 0.4516\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 14.8489 - my_acc: 0.4689\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 13.2412 - my_acc: 0.4582\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 11.8821 - my_acc: 0.4639\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 10.7652 - my_acc: 0.4566\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 9.7687 - my_acc: 0.4566\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.9442 - my_acc: 0.4533\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.2445 - my_acc: 0.4529\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.6163 - my_acc: 0.4529\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.0697 - my_acc: 0.4508\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 6.6185 - my_acc: 0.4541\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.2169 - my_acc: 0.4500\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.8305 - my_acc: 0.4508\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.5135 - my_acc: 0.4533\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.2242 - my_acc: 0.4549\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.9663 - my_acc: 0.4615\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.7303 - my_acc: 0.4689\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 4.5103 - my_acc: 0.4775\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.3189 - my_acc: 0.4803\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.1488 - my_acc: 0.4770\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.9873 - my_acc: 0.4852\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.8380 - my_acc: 0.4848\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.6908 - my_acc: 0.4877\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.5563 - my_acc: 0.4889\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.4475 - my_acc: 0.4918\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.3288 - my_acc: 0.5020\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.2242 - my_acc: 0.5049\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.1264 - my_acc: 0.5131\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.0465 - my_acc: 0.5180\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.9572 - my_acc: 0.5238\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.8732 - my_acc: 0.5172\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.8008 - my_acc: 0.5307\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.7326 - my_acc: 0.5209\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.6616 - my_acc: 0.5291\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.6099 - my_acc: 0.5242\n",
      "Epoch 42/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.5492 - my_acc: 0.5381\n",
      "Epoch 43/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.4877 - my_acc: 0.5389\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.4343 - my_acc: 0.5328\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.3849 - my_acc: 0.5410\n",
      "Epoch 46/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.3356 - my_acc: 0.5430\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2898 - my_acc: 0.5406\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2522 - my_acc: 0.5615\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.2059 - my_acc: 0.5451\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1660 - my_acc: 0.5418\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1354 - my_acc: 0.5357\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0935 - my_acc: 0.5557\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0567 - my_acc: 0.5574\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0370 - my_acc: 0.5463\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9940 - my_acc: 0.5586\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9719 - my_acc: 0.5611\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9445 - my_acc: 0.5561\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9168 - my_acc: 0.5623\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8929 - my_acc: 0.5713\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8648 - my_acc: 0.5643\n",
      "Epoch 61/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8442 - my_acc: 0.5598\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8303 - my_acc: 0.5586\n",
      "Epoch 63/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7992 - my_acc: 0.5635\n",
      "Epoch 64/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7830 - my_acc: 0.5639\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7656 - my_acc: 0.5799\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7478 - my_acc: 0.5648\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.7293 - my_acc: 0.5689\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7114 - my_acc: 0.5684\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7073 - my_acc: 0.5664\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6836 - my_acc: 0.5742\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6665 - my_acc: 0.5713\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6532 - my_acc: 0.5705\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6365 - my_acc: 0.5721\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6234 - my_acc: 0.5766\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.6078 - my_acc: 0.5738\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5977 - my_acc: 0.5713\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5915 - my_acc: 0.5693\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5787 - my_acc: 0.5680\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5633 - my_acc: 0.5775\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5695 - my_acc: 0.5758\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5444 - my_acc: 0.5779\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5393 - my_acc: 0.5754\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.5250 - my_acc: 0.5791\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5136 - my_acc: 0.5738\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5088 - my_acc: 0.5758\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5019 - my_acc: 0.5803\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4843 - my_acc: 0.5754\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4890 - my_acc: 0.5738\n",
      "Epoch 89/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4800 - my_acc: 0.5693\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4721 - my_acc: 0.5689\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4600 - my_acc: 0.5791\n",
      "Epoch 92/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4636 - my_acc: 0.5775\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4461 - my_acc: 0.5779\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4432 - my_acc: 0.5766\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4318 - my_acc: 0.5885\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4282 - my_acc: 0.5709\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4204 - my_acc: 0.5824\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4137 - my_acc: 0.5873\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4121 - my_acc: 0.5734\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4016 - my_acc: 0.5791\n",
      "Epoch 1/100\n",
      "61/61 [==============================] - 1s 2ms/step - loss: 73.1846 - my_acc: 0.3598\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 48.2556 - my_acc: 0.3619\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 34.6965 - my_acc: 0.3820\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 27.0024 - my_acc: 0.3844\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 22.1117 - my_acc: 0.4008\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 18.8810 - my_acc: 0.4000\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 16.5781 - my_acc: 0.4020\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 14.8195 - my_acc: 0.4082\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 13.4164 - my_acc: 0.4217\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 12.2328 - my_acc: 0.4303\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 11.2277 - my_acc: 0.4311\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 10.3908 - my_acc: 0.4311\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 9.6306 - my_acc: 0.4311\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 8.9697 - my_acc: 0.4217\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.3807 - my_acc: 0.4234\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.8621 - my_acc: 0.4143\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.4003 - my_acc: 0.4152\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.9795 - my_acc: 0.4230\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.5998 - my_acc: 0.4225\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.2563 - my_acc: 0.4102\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.9468 - my_acc: 0.4107\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 5.6553 - my_acc: 0.4119\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.3999 - my_acc: 0.4078\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 5.1571 - my_acc: 0.4098\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 4.9441 - my_acc: 0.4033\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 4.7397 - my_acc: 0.4029\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.5476 - my_acc: 0.4029\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 4.3748 - my_acc: 0.4016\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 4.2145 - my_acc: 0.4066\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 4.0631 - my_acc: 0.4082\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.9255 - my_acc: 0.4135\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.7946 - my_acc: 0.4148\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.6812 - my_acc: 0.4148\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.5610 - my_acc: 0.4131\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.4562 - my_acc: 0.4205\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.3547 - my_acc: 0.4176\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.2617 - my_acc: 0.4336\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.1736 - my_acc: 0.4234\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.0869 - my_acc: 0.4398\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.0063 - my_acc: 0.4422\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.9319 - my_acc: 0.4393\n",
      "Epoch 42/100\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.8599 - my_acc: 0.4422\n",
      "Epoch 43/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.7890 - my_acc: 0.4496\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.7258 - my_acc: 0.4512\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.6594 - my_acc: 0.4598\n",
      "Epoch 46/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.5997 - my_acc: 0.4676\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.5486 - my_acc: 0.4664\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.4985 - my_acc: 0.4590\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.4508 - my_acc: 0.4664\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.3963 - my_acc: 0.4783\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.3531 - my_acc: 0.4689\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.3095 - my_acc: 0.4738\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2679 - my_acc: 0.4840\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2277 - my_acc: 0.4779\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1954 - my_acc: 0.4861\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1606 - my_acc: 0.4918\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1234 - my_acc: 0.4967\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0886 - my_acc: 0.4930\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.0545 - my_acc: 0.4988\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.0307 - my_acc: 0.4898\n",
      "Epoch 61/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9976 - my_acc: 0.5020\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9695 - my_acc: 0.5139\n",
      "Epoch 63/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9436 - my_acc: 0.4984\n",
      "Epoch 64/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9165 - my_acc: 0.5090\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.8898 - my_acc: 0.5070\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.8630 - my_acc: 0.5111\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.8401 - my_acc: 0.5049\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.8240 - my_acc: 0.5082\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.7946 - my_acc: 0.5102\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.7706 - my_acc: 0.5164\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.7579 - my_acc: 0.5070\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.7333 - my_acc: 0.5320\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.7164 - my_acc: 0.5082\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.6988 - my_acc: 0.5217\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.6803 - my_acc: 0.5225\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.6666 - my_acc: 0.5217\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.6557 - my_acc: 0.5357\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6349 - my_acc: 0.5320\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.6205 - my_acc: 0.5246\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6072 - my_acc: 0.5262\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5957 - my_acc: 0.5361\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.5852 - my_acc: 0.5250\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.5710 - my_acc: 0.5365\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.5571 - my_acc: 0.5332\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.5536 - my_acc: 0.5242\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.5346 - my_acc: 0.5352\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.5229 - my_acc: 0.5324\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.5162 - my_acc: 0.5402\n",
      "Epoch 89/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.5029 - my_acc: 0.5311\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.4953 - my_acc: 0.5201\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.4830 - my_acc: 0.5426\n",
      "Epoch 92/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.4753 - my_acc: 0.5402\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.4688 - my_acc: 0.5377\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4568 - my_acc: 0.5270\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4514 - my_acc: 0.5242\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.4427 - my_acc: 0.5258\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.4362 - my_acc: 0.5348\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4332 - my_acc: 0.5250\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4195 - my_acc: 0.5316\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.4140 - my_acc: 0.5324\n",
      "Epoch 1/100\n",
      "61/61 [==============================] - 1s 2ms/step - loss: 137.5244 - my_acc: 0.3471\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 106.3908 - my_acc: 0.3680\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 84.8005 - my_acc: 0.3746\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 69.4673 - my_acc: 0.3857\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 57.8563 - my_acc: 0.3922\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 49.0689 - my_acc: 0.3918\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 42.2125 - my_acc: 0.3951\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 36.7822 - my_acc: 0.4020\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 32.4840 - my_acc: 0.4045\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 28.9225 - my_acc: 0.3922\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 26.0066 - my_acc: 0.4041\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 23.5863 - my_acc: 0.4004\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 21.5178 - my_acc: 0.4041\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 19.8021 - my_acc: 0.3963\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 18.3332 - my_acc: 0.4020\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 17.0621 - my_acc: 0.4012\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 15.9297 - my_acc: 0.4107\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 14.9158 - my_acc: 0.4205\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 14.0333 - my_acc: 0.4254\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 13.2238 - my_acc: 0.4270\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 12.5162 - my_acc: 0.4246\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 11.8421 - my_acc: 0.4176\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 11.2489 - my_acc: 0.4201\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 10.7005 - my_acc: 0.4287\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 10.2004 - my_acc: 0.4299\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 9.7121 - my_acc: 0.4307\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 9.2741 - my_acc: 0.4348\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 8.8669 - my_acc: 0.4357\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 8.4915 - my_acc: 0.4332\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 8.1347 - my_acc: 0.4361\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.8057 - my_acc: 0.4385\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 7.4911 - my_acc: 0.4369\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 7.1993 - my_acc: 0.4352\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.9249 - my_acc: 0.4398\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.6667 - my_acc: 0.4377\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.4320 - my_acc: 0.4332\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.1921 - my_acc: 0.4385\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 5.9757 - my_acc: 0.4414\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 5.7667 - my_acc: 0.4439\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 5.5706 - my_acc: 0.4402\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 5.3789 - my_acc: 0.4451\n",
      "Epoch 42/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 5.2014 - my_acc: 0.4439\n",
      "Epoch 43/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 5.0319 - my_acc: 0.4496\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.8609 - my_acc: 0.4492\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 4.7070 - my_acc: 0.4512\n",
      "Epoch 46/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 4.5616 - my_acc: 0.4545\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 4.4121 - my_acc: 0.4533\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 4.2757 - my_acc: 0.4500\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 4.1487 - my_acc: 0.4549\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.0237 - my_acc: 0.4561\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.9081 - my_acc: 0.4586\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.7971 - my_acc: 0.4557\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.6952 - my_acc: 0.4586\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.5966 - my_acc: 0.4611\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.5010 - my_acc: 0.4549\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.4111 - my_acc: 0.4553\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.3269 - my_acc: 0.4574\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.2478 - my_acc: 0.4701\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.1673 - my_acc: 0.4668\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 3.0894 - my_acc: 0.4656\n",
      "Epoch 61/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.0198 - my_acc: 0.4766\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.9532 - my_acc: 0.4783\n",
      "Epoch 63/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.8868 - my_acc: 0.4816\n",
      "Epoch 64/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.8240 - my_acc: 0.4848\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.7658 - my_acc: 0.4906\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.7031 - my_acc: 0.4914\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.6465 - my_acc: 0.4889\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.5970 - my_acc: 0.4906\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.5430 - my_acc: 0.5016\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.4972 - my_acc: 0.5074\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.4509 - my_acc: 0.5012\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.4084 - my_acc: 0.5131\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.3626 - my_acc: 0.5016\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.3203 - my_acc: 0.5086\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.2860 - my_acc: 0.5123\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2426 - my_acc: 0.5139\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.2066 - my_acc: 0.5197\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1694 - my_acc: 0.5250\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1317 - my_acc: 0.5291\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.0991 - my_acc: 0.5275\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0712 - my_acc: 0.5279\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0384 - my_acc: 0.5299\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0052 - my_acc: 0.5434\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.9806 - my_acc: 0.5414\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9530 - my_acc: 0.5279\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9249 - my_acc: 0.5393\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9012 - my_acc: 0.5340\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8721 - my_acc: 0.5447\n",
      "Epoch 89/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8525 - my_acc: 0.5357\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.8294 - my_acc: 0.5373\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.8058 - my_acc: 0.5389\n",
      "Epoch 92/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7804 - my_acc: 0.5414\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.7598 - my_acc: 0.5381\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.7408 - my_acc: 0.5381\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.7213 - my_acc: 0.5402\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.7044 - my_acc: 0.5451\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6883 - my_acc: 0.5443\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 1.6677 - my_acc: 0.5459\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.6508 - my_acc: 0.5422\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.6360 - my_acc: 0.5525\n",
      "Epoch 1/100\n",
      "61/61 [==============================] - 1s 2ms/step - loss: 125.0566 - my_acc: 0.3799\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 85.2273 - my_acc: 0.3791\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 61.7159 - my_acc: 0.4074\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 47.4105 - my_acc: 0.4328\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 38.2077 - my_acc: 0.4467\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 31.8976 - my_acc: 0.4492\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 27.4340 - my_acc: 0.4426\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 24.1636 - my_acc: 0.4488\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 21.6512 - my_acc: 0.4492\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 19.6438 - my_acc: 0.4578\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 18.0359 - my_acc: 0.4660\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 16.6927 - my_acc: 0.4611\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 15.5409 - my_acc: 0.4602\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 14.5293 - my_acc: 0.4619\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 13.6476 - my_acc: 0.4566\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 12.8393 - my_acc: 0.4693\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 12.1281 - my_acc: 0.4545\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 11.4640 - my_acc: 0.4504\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 10.8646 - my_acc: 0.4557\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 10.3034 - my_acc: 0.4545\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 9.7953 - my_acc: 0.4533\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 9.2823 - my_acc: 0.4574\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.8446 - my_acc: 0.4689\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 8.4182 - my_acc: 0.4730\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 8.0369 - my_acc: 0.4857\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 7.6819 - my_acc: 0.4926\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 7.3438 - my_acc: 0.4865\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 7.0246 - my_acc: 0.4910\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 6.7468 - my_acc: 0.5053\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 6.4836 - my_acc: 0.5033\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 6.2331 - my_acc: 0.5102\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 5.9893 - my_acc: 0.5156\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 5.7736 - my_acc: 0.5205\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 5.5691 - my_acc: 0.5234\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 5.3869 - my_acc: 0.5270\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 5.2175 - my_acc: 0.5299\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 5.0368 - my_acc: 0.5303\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.8840 - my_acc: 0.5369\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.7404 - my_acc: 0.5373\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 4.6187 - my_acc: 0.5402\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 4.4771 - my_acc: 0.5418\n",
      "Epoch 42/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.3616 - my_acc: 0.5545\n",
      "Epoch 43/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 4.2470 - my_acc: 0.5578\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 4.1584 - my_acc: 0.5496\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 4.0596 - my_acc: 0.5541\n",
      "Epoch 46/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.9520 - my_acc: 0.5582\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.8594 - my_acc: 0.5639\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.7644 - my_acc: 0.5590\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.6847 - my_acc: 0.5660\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.6075 - my_acc: 0.5779\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.5314 - my_acc: 0.5689\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.4575 - my_acc: 0.5689\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.3880 - my_acc: 0.5689\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.3242 - my_acc: 0.5746\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.2610 - my_acc: 0.5734\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.2040 - my_acc: 0.5828\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.1514 - my_acc: 0.5807\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.0936 - my_acc: 0.5795\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 3.0315 - my_acc: 0.5824\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.9840 - my_acc: 0.5824\n",
      "Epoch 61/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.9287 - my_acc: 0.5811\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.8823 - my_acc: 0.5758\n",
      "Epoch 63/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.8351 - my_acc: 0.5705\n",
      "Epoch 64/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.7971 - my_acc: 0.5828\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.7502 - my_acc: 0.5824\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.7131 - my_acc: 0.5865\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.6686 - my_acc: 0.5959\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.6414 - my_acc: 0.5881\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.5988 - my_acc: 0.5840\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.5608 - my_acc: 0.5848\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.5307 - my_acc: 0.5848\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.4937 - my_acc: 0.5947\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.4637 - my_acc: 0.5918\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.4406 - my_acc: 0.5943\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.4043 - my_acc: 0.5951\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.3778 - my_acc: 0.5861\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.3524 - my_acc: 0.5865\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.3195 - my_acc: 0.5939\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2980 - my_acc: 0.5898\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.2671 - my_acc: 0.5893\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.2508 - my_acc: 0.5914\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.2231 - my_acc: 0.5795\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1965 - my_acc: 0.5980\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1727 - my_acc: 0.5832\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1593 - my_acc: 0.5877\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1295 - my_acc: 0.5889\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1239 - my_acc: 0.5857\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1009 - my_acc: 0.5852\n",
      "Epoch 89/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0759 - my_acc: 0.5881\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0546 - my_acc: 0.5857\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.0380 - my_acc: 0.5828\n",
      "Epoch 92/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0312 - my_acc: 0.5865\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.0031 - my_acc: 0.5877\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9920 - my_acc: 0.5824\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9796 - my_acc: 0.5869\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9622 - my_acc: 0.5906\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9504 - my_acc: 0.5861\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9381 - my_acc: 0.5889\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9254 - my_acc: 0.5910\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.9085 - my_acc: 0.5934\n"
     ]
    }
   ],
   "source": [
    "dconf_size = 10\n",
    "bootstrap_step_size = int(train_data.shape[0] / dconf_size)\n",
    "models = []\n",
    "for x in range(dconf_size):\n",
    "    X_boot, y_boot = X[x*bootstrap_step_size:(x+1)*bootstrap_step_size], y[x*bootstrap_step_size:(x+1)*bootstrap_step_size]\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(14))\n",
    "    model.add(Dense(16, activation=\"relu\", kernel_initializer=initializers.RandomNormal(stddev=1), bias_initializer=initializers.Zeros()))\n",
    "    model.add(Dense(2, activation=\"linear\", kernel_initializer=initializers.RandomNormal(stddev=1), bias_initializer=initializers.Zeros()))\n",
    "    # callback = callbacks.EarlyStopping(monitor='my_acc', patience=10, mode=\"max\")\n",
    "    # callback = callbacks.EarlyStopping(monitor='loss', patience=2, mode=\"min\")\n",
    "    model.compile(loss=my_loss, optimizer='adam', metrics=my_acc)\n",
    "    hist = model.fit(X_boot, y_boot, epochs=100, batch_size=10, shuffle=True)#, callbacks=[callback])\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in range(dconf_size):\n",
    "    preds.append(models[i](X_test))\n",
    "\n",
    "preds = np.stack(preds)\n",
    "predictions_home = np.swapaxes(preds, 0, 1)[:, :, 0]\n",
    "predictions_away = np.swapaxes(preds, 0, 1)[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_res = []\n",
    "for h, a in y_test:\n",
    "    act_res.append(f\"{str(int(h))}:{str(int(a))}\")\n",
    "\n",
    "predictions_home = np.swapaxes(preds, 0, 1)[:, :, 0]\n",
    "predictions_away = np.swapaxes(preds, 0, 1)[:, :, 1]\n",
    "\n",
    "game_quotes = []\n",
    "most_goals = {\"home\": [], \"away\": []}\n",
    "for game_idx in range(len(predictions_home)):\n",
    "    game_df = pd.DataFrame(\n",
    "        {\"home\": predictions_home[game_idx], \"away\": predictions_away[game_idx]}\n",
    "    )\n",
    "    game_df[\"diff\"] = game_df[\"home\"] - game_df[\"away\"]\n",
    "    game_df[\"clipped_res\"] = np.clip(game_df[\"diff\"], -1, 1)\n",
    "    game_df[\"rounded_res\"] = np.rint(game_df[\"clipped_res\"])\n",
    "    home = game_df.loc[game_df[\"rounded_res\"] == 1].shape[0] / dconf_size\n",
    "    draw = (\n",
    "        game_df.loc[game_df[\"rounded_res\"] == 0].shape[0] / dconf_size\n",
    "    )\n",
    "    away = game_df.loc[game_df[\"rounded_res\"] == -1].shape[0] / dconf_size\n",
    "\n",
    "    game_quotes.append(f\"{round(home, 3)}-{round(draw, 3)}-{round(away, 3)}\")\n",
    "df_res = pd.DataFrame(\n",
    "    {\"actual\": act_res, \"predicted\": game_quotes}\n",
    ")\n",
    "df_cross = pd.DataFrame(\n",
    "    {\n",
    "        \"actual\": [\n",
    "            0\n",
    "            if int(df_res.iloc[i][\"actual\"].split(\":\")[0])\n",
    "            > int(df_res.iloc[i][\"actual\"].split(\":\")[1])\n",
    "            else 1\n",
    "            if int(df_res.iloc[i][\"actual\"].split(\":\")[0])\n",
    "            == int(df_res.iloc[i][\"actual\"].split(\":\")[1])\n",
    "            else 2\n",
    "            for i in range(df_res.shape[0])\n",
    "        ],\n",
    "        \"pred\": [\n",
    "            np.argmax([float(y) for y in df_res.iloc[i][\"predicted\"].split(\"-\")])\n",
    "            for i in range(df_res.shape[0])\n",
    "        ],\n",
    "        \"pred_val\": [\n",
    "            np.max([float(y) for y in df_res.iloc[i][\"predicted\"].split(\"-\")])\n",
    "            for i in range(df_res.shape[0])\n",
    "        ],\n",
    "        \"bookie\": [\n",
    "            np.argmax(\n",
    "                [\n",
    "                    test_data.iloc[i].bookie_home,\n",
    "                    test_data.iloc[i].bookie_draw,\n",
    "                    test_data.iloc[i].bookie_away,\n",
    "                ]\n",
    "            )\n",
    "            for i in range(test_data.shape[0])\n",
    "        ],\n",
    "        \"bookie_val\": [\n",
    "            np.max(\n",
    "                [\n",
    "                    test_data.iloc[i].bookie_home,\n",
    "                    test_data.iloc[i].bookie_draw,\n",
    "                    test_data.iloc[i].bookie_away,\n",
    "                ]\n",
    "            )\n",
    "            for i in range(test_data.shape[0])\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verteilung:\n",
      "Anzahl Predicted Home: 756 (48.93%) | Verteilung Tatsächlich Home: 42.78%\n",
      "Anzahl Predicted Draw: 397 (25.7%) | Verteilung Tatsächlich Draw: 25.5%\n",
      "Anzahl Predicted Away: 392 (25.37%) | Verteilung Tatsächlich Away: 31.72%\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction:\n",
      "Anzahl Korrekt: 849 (54.95%), Anzahl Falsch: 696 (45.05%)\n",
      "---------------------------------------------------------------------------------\n",
      "Anzahl Home Korrekt: 470 (71.1%), Anzahl Home Falsch: 286\n",
      "Home Pred. aber Draw --> 171\n",
      "Home Pred. aber Away --> 115\n",
      "Anzahl Draw Korrekt: 134 (34.01%), Anzahl Draw Falsch: 263\n",
      "Draw Pred. aber Home --> 133\n",
      "Draw Pred. aber Away --> 130\n",
      "Anzahl Away Korrekt: 245 (50.0%), Anzahl Away Falsch: 147\n",
      "Away Pred. aber Home --> 58\n",
      "Away Pred. aber Draw --> 89\n"
     ]
    }
   ],
   "source": [
    "print(\"Verteilung:\")\n",
    "print(\n",
    "    f\"Anzahl Predicted Home: {df_cross[df_cross['pred'] == 0].shape[0]} ({round(df_cross[df_cross['pred'] == 0].shape[0] / df_cross.shape[0] * 100, 2)}%)\",\n",
    "    end=\"\",\n",
    ")\n",
    "print(\n",
    "    f\" | Verteilung Tatsächlich Home: {round(df_cross[df_cross['actual'] == 0].shape[0] / df_cross.shape[0] * 100, 2)}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Anzahl Predicted Draw: {df_cross[df_cross['pred'] == 1].shape[0]} ({round(df_cross[df_cross['pred'] == 1].shape[0] / df_cross.shape[0] * 100, 2)}%)\",\n",
    "    end=\"\",\n",
    ")\n",
    "print(\n",
    "    f\" | Verteilung Tatsächlich Draw: {round(df_cross[df_cross['actual'] == 1].shape[0] / df_cross.shape[0] * 100, 2)}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Anzahl Predicted Away: {df_cross[df_cross['pred'] == 2].shape[0]} ({round(df_cross[df_cross['pred'] == 2].shape[0] / df_cross.shape[0] * 100, 2)}%)\",\n",
    "    end=\"\",\n",
    ")\n",
    "print(\n",
    "    f\" | Verteilung Tatsächlich Away: {round(df_cross[df_cross['actual'] == 2].shape[0] / df_cross.shape[0] * 100, 2)}%\"\n",
    ")\n",
    "print(\n",
    "    \"---------------------------------------------------------------------------------\"\n",
    ")\n",
    "print(\n",
    "    \"---------------------------------------------------------------------------------\"\n",
    ")\n",
    "right, wrong, home_right, draw_right, away_right = 0, 0, 0, 0, 0\n",
    "home_pred = {\"act_away\": 0, \"act_draw\": 0}\n",
    "draw_pred = {\"act_away\": 0, \"act_home\": 0}\n",
    "away_pred = {\"act_home\": 0, \"act_draw\": 0}\n",
    "for x in range(df_cross.shape[0]):\n",
    "    if df_cross.iloc[x][\"actual\"] != df_cross.iloc[x][\"pred\"]:\n",
    "        if df_cross.iloc[x][\"pred\"] == 0:\n",
    "            if df_cross.iloc[x][\"actual\"] == 1:\n",
    "                home_pred[\"act_draw\"] += 1\n",
    "            else:\n",
    "                home_pred[\"act_away\"] += 1\n",
    "        elif df_cross.iloc[x][\"pred\"] == 1:\n",
    "            if df_cross.iloc[x][\"actual\"] == 0:\n",
    "                draw_pred[\"act_home\"] += 1\n",
    "            else:\n",
    "                draw_pred[\"act_away\"] += 1\n",
    "        else:\n",
    "            if df_cross.iloc[x][\"actual\"] == 0:\n",
    "                away_pred[\"act_home\"] += 1\n",
    "            else:\n",
    "                away_pred[\"act_draw\"] += 1\n",
    "        wrong += 1\n",
    "    else:\n",
    "        if df_cross.iloc[x][\"actual\"] == 0:\n",
    "            home_right += 1\n",
    "        elif df_cross.iloc[x][\"actual\"] == 1:\n",
    "            draw_right += 1\n",
    "        else:\n",
    "            away_right += 1\n",
    "        right += 1\n",
    "\n",
    "home_wrong = home_pred[\"act_away\"] + home_pred[\"act_draw\"]\n",
    "draw_wrong = draw_pred[\"act_away\"] + draw_pred[\"act_home\"]\n",
    "away_wrong = away_pred[\"act_home\"] + away_pred[\"act_draw\"]\n",
    "print(\"Prediction:\")\n",
    "print(\n",
    "    f\"Anzahl Korrekt: {right} ({round(right / df_cross.shape[0] * 100,2)}%), Anzahl Falsch: {wrong} ({round(wrong / df_cross.shape[0] * 100, 2)}%)\"\n",
    ")\n",
    "print(\n",
    "    \"---------------------------------------------------------------------------------\"\n",
    ")\n",
    "print(\n",
    "    f\"Anzahl Home Korrekt: {home_right} ({round(home_right / df_cross[df_cross['actual'] == 0].shape[0] * 100, 2)}%), Anzahl Home Falsch: {home_wrong}\"\n",
    ")\n",
    "print(f\"Home Pred. aber Draw --> {home_pred['act_draw']}\")\n",
    "print(f\"Home Pred. aber Away --> {home_pred['act_away']}\")\n",
    "print(\n",
    "    f\"Anzahl Draw Korrekt: {draw_right} ({round(draw_right / df_cross[df_cross['actual'] == 1].shape[0] * 100, 2)}%), Anzahl Draw Falsch: {draw_wrong}\"\n",
    ")\n",
    "print(f\"Draw Pred. aber Home --> {draw_pred['act_home']}\")\n",
    "print(f\"Draw Pred. aber Away --> {draw_pred['act_away']}\")\n",
    "print(\n",
    "    f\"Anzahl Away Korrekt: {away_right} ({round(away_right / df_cross[df_cross['actual'] == 2].shape[0] * 100, 2)}%), Anzahl Away Falsch: {away_wrong}\"\n",
    ")\n",
    "print(f\"Away Pred. aber Home --> {away_pred['act_home']}\")\n",
    "print(f\"Away Pred. aber Draw --> {away_pred['act_draw']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1:1</td>\n",
       "      <td>0.8-0.2-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1:0</td>\n",
       "      <td>0.6-0.2-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2:0</td>\n",
       "      <td>0.9-0.1-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2:1</td>\n",
       "      <td>1.0-0.0-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0:1</td>\n",
       "      <td>0.2-0.7-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2:1</td>\n",
       "      <td>0.6-0.4-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1:1</td>\n",
       "      <td>0.0-0.0-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2:2</td>\n",
       "      <td>0.6-0.3-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1:2</td>\n",
       "      <td>0.1-0.2-0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2:1</td>\n",
       "      <td>0.1-0.1-0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1:0</td>\n",
       "      <td>0.7-0.2-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2:0</td>\n",
       "      <td>0.9-0.1-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4:0</td>\n",
       "      <td>1.0-0.0-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2:1</td>\n",
       "      <td>0.8-0.2-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2:2</td>\n",
       "      <td>0.3-0.2-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2:0</td>\n",
       "      <td>0.8-0.2-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0:2</td>\n",
       "      <td>0.2-0.7-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1:1</td>\n",
       "      <td>0.9-0.0-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2:1</td>\n",
       "      <td>0.3-0.5-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1:3</td>\n",
       "      <td>0.0-0.1-0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual    predicted\n",
       "0     1:1  0.8-0.2-0.0\n",
       "1     1:0  0.6-0.2-0.2\n",
       "2     2:0  0.9-0.1-0.0\n",
       "3     2:1  1.0-0.0-0.0\n",
       "4     0:1  0.2-0.7-0.1\n",
       "5     2:1  0.6-0.4-0.0\n",
       "6     1:1  0.0-0.0-1.0\n",
       "7     2:2  0.6-0.3-0.1\n",
       "8     1:2  0.1-0.2-0.7\n",
       "9     2:1  0.1-0.1-0.8\n",
       "10    1:0  0.7-0.2-0.1\n",
       "11    2:0  0.9-0.1-0.0\n",
       "12    4:0  1.0-0.0-0.0\n",
       "13    2:1  0.8-0.2-0.0\n",
       "14    2:2  0.3-0.2-0.5\n",
       "15    2:0  0.8-0.2-0.0\n",
       "16    0:2  0.2-0.7-0.1\n",
       "17    1:1  0.9-0.0-0.1\n",
       "18    2:1  0.3-0.5-0.2\n",
       "19    1:3  0.0-0.1-0.9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Std analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home std: 0.8449105620384216, Away std: 0.6915190815925598, Overall std: 0.7682148218154907\n"
     ]
    }
   ],
   "source": [
    "home_std = np.std(predictions_home, axis=1)\n",
    "away_std = np.std(predictions_away, axis=1)\n",
    "print(f\"Home std: {home_std.mean()}, Away std: {away_std.mean()}, Overall std: {(home_std.mean() + away_std.mean()) / 2}\")\n",
    "# Draw std Boxplots\n",
    "if False:\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 2)\n",
    "    fig.set_size_inches(10, 10)\n",
    "    ax1[0].scatter(home_std, np.arange(1545))\n",
    "    ax2[0].scatter(away_std, np.arange(1545))\n",
    "    ax1[1].boxplot(home_std)\n",
    "    ax2[1].boxplot(away_std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicted Goals analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed: Home: 1.5348569549490299, away: 1.2387372574810918\n",
      "Predicted: Home:1.1073398109243697, Away: 0.8179979117723832\n"
     ]
    }
   ],
   "source": [
    "home_histograms, away_histograms = [], []\n",
    "for game_idx in range(len(predictions_home)):\n",
    "    home_hist, bin_edges = np.histogram(\n",
    "        np.array(predictions_home[game_idx], dtype=int), [0, 1, 2, 3, 4, 5, 6]\n",
    "    )\n",
    "    away_hist, bin_edges = np.histogram(\n",
    "        np.array(predictions_away[game_idx], dtype=int), [0, 1, 2, 3, 4, 5, 6]\n",
    "    )\n",
    "    home_histograms.append(home_hist)\n",
    "    away_histograms.append(away_hist)\n",
    "\n",
    "home_cum_hist = np.sum(home_histograms, axis=0)\n",
    "away_cum_hist = np.sum(away_histograms, axis=0)\n",
    "\n",
    "if False: # draw histograms\n",
    "    his_h, _ = np.histogram(train_data.home_score, [0, 1, 2, 3, 4, 5, 6])\n",
    "    his_a, _ = np.histogram(train_data.away_score, [0, 1, 2, 3, 4, 5, 6])\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 2)\n",
    "    fig.set_size_inches(10, 10)\n",
    "    fig.suptitle(\"Independent Poisson Model\", fontsize=16)\n",
    "    ax1[0].bar(np.arange(len(his_h)), his_h / np.sum(his_h), color=\"lightskyblue\")\n",
    "    ax1[0].set_title(\"Home Observed\")\n",
    "    ax1[0].set_xlabel(\"Number of Goals\")\n",
    "    ax1[0].set_ylabel(\"Probability of observed goal count\")\n",
    "    ax1[0].get_yaxis().set_major_formatter(\n",
    "        matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x * 100), \",\") + \"%\")\n",
    "    )\n",
    "    ax1[0].grid(axis=\"y\")\n",
    "\n",
    "    ax2[0].bar(\n",
    "        np.arange(len(home_cum_hist)),\n",
    "        home_cum_hist / np.sum(home_cum_hist),\n",
    "        color=\"lightcoral\",\n",
    "    )\n",
    "    ax2[0].set_title(\"Home Predicted\")\n",
    "    ax2[0].set_xlabel(\"Number of Goals\")\n",
    "    ax2[0].set_ylabel(\"Probability of observed goal count\")\n",
    "    ax2[0].get_yaxis().set_major_formatter(\n",
    "        matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x * 100), \",\") + \"%\")\n",
    "    )\n",
    "    ax2[0].grid(axis=\"y\")\n",
    "\n",
    "    ax1[1].bar(np.arange(len(his_a)), his_a / np.sum(his_a), color=\"lightskyblue\")\n",
    "    ax1[1].set_title(\"Away Observed\")\n",
    "    ax1[1].set_xlabel(\"Number of Goals\")\n",
    "    ax1[1].set_ylabel(\"Probability of observed goal count\")\n",
    "    ax1[1].get_yaxis().set_major_formatter(\n",
    "        matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x * 100), \",\") + \"%\")\n",
    "    )\n",
    "    ax1[1].grid(axis=\"y\")\n",
    "\n",
    "    ax2[1].bar(\n",
    "        np.arange(len(away_cum_hist)),\n",
    "        away_cum_hist / np.sum(away_cum_hist),\n",
    "        color=\"lightcoral\",\n",
    "    )\n",
    "    ax2[1].set_title(\"Away Predicted\")\n",
    "    ax2[1].set_xlabel(\"Number of Goals\")\n",
    "    ax2[1].set_ylabel(\"Probability of observed goal count\")\n",
    "    ax2[1].get_yaxis().set_major_formatter(\n",
    "        matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x * 100), \",\") + \"%\")\n",
    "    )\n",
    "    ax2[1].grid(axis=\"y\")\n",
    "\n",
    "print(f\"Observed: Home: {np.mean(train_data.home_score)}, away: {np.mean(train_data.away_score)}\")\n",
    "print(f\"Predicted: Home:{np.sum([x * idx for idx, x in enumerate(home_cum_hist)]) / np.sum(home_cum_hist)}, Away: {np.sum([x * idx for idx, x in enumerate(away_cum_hist)]) / np.sum(away_cum_hist)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result HEatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # actual\n",
    "    max_goals = int(np.max(y_test)) + 1\n",
    "    res_table_a = np.array([np.zeros(max_goals) for _ in range(max_goals)])\n",
    "    for h, a in y_test:\n",
    "        res_table_a[int(h)][int(a)] += 1\n",
    "    res_table_a = res_table_a / test_data.shape[0]\n",
    "\n",
    "    # predicted\n",
    "    max_goals = int(np.max(y_test)) + 1\n",
    "    res_table_p = np.array([np.zeros(max_goals) for _ in range(max_goals)])\n",
    "\n",
    "    game_quotes = []\n",
    "    for game_idx in range(len(predictions_home)):\n",
    "        home_hist, bin_edges = np.histogram(\n",
    "            predictions_home[game_idx], [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "        )\n",
    "        away_hist, bin_edges = np.histogram(\n",
    "            predictions_away[game_idx], [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "        )\n",
    "        if home_hist.shape[0] < max_goals:\n",
    "            home_hist = np.append(home_hist, np.zeros(max_goals - home_hist.shape[0]))\n",
    "        if away_hist.shape[0] < max_goals:\n",
    "            away_hist = np.append(away_hist, np.zeros(max_goals - away_hist.shape[0]))\n",
    "\n",
    "        home_hist = home_hist / dconf_size\n",
    "        away_hist = away_hist / dconf_size\n",
    "        probs = home_hist.reshape(home_hist.shape[0], 1) * away_hist\n",
    "        probs = probs\n",
    "        res_table_p += probs\n",
    "\n",
    "    res_table_p = res_table_p / test_data.shape[0]\n",
    "\n",
    "    # sb.heatmap(res_table_p, annot=True)\n",
    "\n",
    "    # top pred result:\n",
    "    max_goals = int(np.max(y_test)) + 1\n",
    "    res_table_pr = np.array([np.zeros(max_goals) for _ in range(max_goals)])\n",
    "\n",
    "    game_quotes = []\n",
    "    for game_idx in range(len(predictions_home)):\n",
    "        home_hist, bin_edges = np.histogram(\n",
    "            predictions_home[game_idx], [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "        )\n",
    "        away_hist, bin_edges = np.histogram(\n",
    "            predictions_away[game_idx], [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "        )\n",
    "        res_table_pr[np.argmax(home_hist)][np.argmax(away_hist)] += 1\n",
    "    res_table_pr = res_table_pr / test_data.shape[0]\n",
    "    # sb.heatmap(res_table_pr, annot=True)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12.5, 5))\n",
    "    fig.suptitle(\"Independent Poisson Model\")\n",
    "    sb.heatmap(ax=axes[0], data=res_table_a, annot=True, fmt=\".2f\")\n",
    "    axes[0].set_title(\"Observed\")\n",
    "    sb.heatmap(ax=axes[1], data=res_table_p, annot=True, fmt=\".2f\")\n",
    "    axes[1].set_title(\"Predicted\")\n",
    "\n",
    "    print(f\"Observed: Over: {np.tril(res_table_a).sum() - np.trace(res_table_a)}, Diagonal: {np.trace(res_table_a)}, Under: {np.triu(res_table_a).sum() - np.trace(res_table_a)}\")\n",
    "    print(f\"Predicted: Over: {np.tril(res_table_p).sum() - np.trace(res_table_p)}, Diagonal: {np.trace(res_table_p)}, Under: {np.triu(res_table_p).sum() - np.trace(res_table_p)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted ECE Home: 0.04027887840293287\n",
      "Predicted ECE Away: 0.2450370162001816\n",
      "Predicted ECE Away: 0.2556122448979593\n"
     ]
    }
   ],
   "source": [
    "# ece\n",
    "data_length = df_cross.shape[0]\n",
    "acc_home = (\n",
    "    df_cross.loc[(df_cross[\"pred\"] == 0) & (df_cross[\"actual\"] == 0)].shape[0]\n",
    ") / df_cross.loc[df_cross[\"actual\"] == 0].shape[0]\n",
    "acc_draw = (\n",
    "    df_cross.loc[(df_cross[\"pred\"] == 1) & (df_cross[\"actual\"] == 1)].shape[0]\n",
    ") / df_cross.loc[df_cross[\"actual\"] == 1].shape[0]\n",
    "acc_away = (\n",
    "    df_cross.loc[(df_cross[\"pred\"] == 2) & (df_cross[\"actual\"] == 2)].shape[0]\n",
    ") / df_cross.loc[df_cross[\"actual\"] == 2].shape[0]\n",
    "\n",
    "conf_home = np.mean(df_cross.loc[(df_cross[\"pred\"] == 0)].pred_val)\n",
    "conf_draw = np.mean(df_cross.loc[(df_cross[\"pred\"] == 1)].pred_val)\n",
    "conf_away = np.mean(df_cross.loc[(df_cross[\"pred\"] == 2)].pred_val)\n",
    "print(f\"Predicted ECE Home: {abs(acc_home - conf_home)}\")\n",
    "print(f\"Predicted ECE Away: {abs(acc_draw - conf_draw)}\")\n",
    "print(f\"Predicted ECE Away: {abs(acc_away - conf_away)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bets: 1577\n",
      "won: 817\n",
      "lost: 760\n",
      "money:  4893.500000000005\n"
     ]
    }
   ],
   "source": [
    "bet_threshold = 10\n",
    "bets, bets_won, bets_lost = 0, 0, 0\n",
    "risk, money = 10, 0\n",
    "for idx, x in df_res.iterrows():\n",
    "    res = x.actual.split(\":\")\n",
    "    result = 0 if res[0] > res[1] else 1 if res[0] == res[1] else 2\n",
    "    pred = x.predicted.split(\"-\")\n",
    "    pred_h, pred_d, pred_a = pred[0], pred[1], pred[2]\n",
    "    bookie_h, bookie_d, bookie_a = (\n",
    "        test_data.iloc[idx].bookie_home,\n",
    "        test_data.iloc[idx].bookie_draw,\n",
    "        test_data.iloc[idx].bookie_away,\n",
    "    )\n",
    "    bookie_h_odd, bookie_d_odd, bookie_a_odd = (\n",
    "        test_data.iloc[idx].bookie_home_odd,\n",
    "        test_data.iloc[idx].bookie_draw_odd,\n",
    "        test_data.iloc[idx].bookie_away_odd,\n",
    "    )\n",
    "    if (float(pred_h) * 100) - bookie_h > bet_threshold:\n",
    "        bets += 1\n",
    "        if result == 0:\n",
    "            bets_won += 1\n",
    "            money += risk * (bookie_h_odd) - risk\n",
    "        else:\n",
    "            bets_lost += 1\n",
    "            money -= risk\n",
    "    if (float(pred_d) * 100) - bookie_d > bet_threshold:\n",
    "        bets += 1\n",
    "        if result == 1:\n",
    "            bets_won += 1\n",
    "            money += risk * (bookie_d_odd) - risk\n",
    "        else:\n",
    "            bets_lost += 1\n",
    "            money -= risk\n",
    "    if (float(pred_a) * 100) - bookie_a > bet_threshold:\n",
    "        bets += 1\n",
    "        if result == 2:\n",
    "            bets_won += 1\n",
    "            money += risk * (bookie_a_odd) - risk\n",
    "        else:\n",
    "            bets_lost += 1\n",
    "            money -= risk\n",
    "\n",
    "print(f\"bets: {bets}\")\n",
    "print(f\"won: {bets_won}\")\n",
    "print(f\"lost: {bets_lost}\")\n",
    "print(\"money: \", money)\n",
    "\n",
    "# bets: 1004\n",
    "# won: 467\n",
    "# lost: 537\n",
    "# money:  6749.102471962487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "auswertung = pd.read_csv(\"/home/morten/Downloads/MLP_size - probabilistic.tsv\", sep=\"\\t\").replace(\",\", \".\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dropout</th>\n",
       "      <th>link</th>\n",
       "      <th>acc</th>\n",
       "      <th>std</th>\n",
       "      <th>ece_h</th>\n",
       "      <th>eve_d</th>\n",
       "      <th>ece_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>controlled</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.4537</td>\n",
       "      <td>1.6427</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.2337</td>\n",
       "      <td>0.2874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>controlled</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.4466</td>\n",
       "      <td>1.3414</td>\n",
       "      <td>0.1081</td>\n",
       "      <td>0.1229</td>\n",
       "      <td>0.2539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>controlled</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.4634</td>\n",
       "      <td>1.4758</td>\n",
       "      <td>0.1013</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>0.1519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>controlled</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.4738</td>\n",
       "      <td>2.8651</td>\n",
       "      <td>0.0913</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.1732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>controlled</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.4414</td>\n",
       "      <td>1.0447</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.2725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>controlled</td>\n",
       "      <td>pow</td>\n",
       "      <td>0.4913</td>\n",
       "      <td>2.9921</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.3652</td>\n",
       "      <td>0.1345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>controlled</td>\n",
       "      <td>pow</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>1.2842</td>\n",
       "      <td>0.0746</td>\n",
       "      <td>0.0629</td>\n",
       "      <td>0.2415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>controlled</td>\n",
       "      <td>pow</td>\n",
       "      <td>0.4427</td>\n",
       "      <td>3.6516</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.2783</td>\n",
       "      <td>0.2793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>controlled</td>\n",
       "      <td>pow</td>\n",
       "      <td>0.3793</td>\n",
       "      <td>1.6634</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.3545</td>\n",
       "      <td>0.2503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>controlled</td>\n",
       "      <td>pow</td>\n",
       "      <td>0.5023</td>\n",
       "      <td>1.5647</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.2792</td>\n",
       "      <td>0.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.25</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.5631</td>\n",
       "      <td>0.2587</td>\n",
       "      <td>0.1686</td>\n",
       "      <td>0.2988</td>\n",
       "      <td>0.2766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.25</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.5663</td>\n",
       "      <td>0.2719</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.2992</td>\n",
       "      <td>0.2638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.25</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.5566</td>\n",
       "      <td>0.2611</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.3051</td>\n",
       "      <td>0.2708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.25</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.2659</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.3058</td>\n",
       "      <td>0.2676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.25</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.2633</td>\n",
       "      <td>0.2045</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.2765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.5</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.5346</td>\n",
       "      <td>0.3057</td>\n",
       "      <td>0.1805</td>\n",
       "      <td>0.1481</td>\n",
       "      <td>0.2765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.5</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.5113</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>0.1483</td>\n",
       "      <td>0.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.5</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.5301</td>\n",
       "      <td>0.3042</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1532</td>\n",
       "      <td>0.3412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.5</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.5417</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.1152</td>\n",
       "      <td>0.1681</td>\n",
       "      <td>0.3173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.5</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.5359</td>\n",
       "      <td>0.3133</td>\n",
       "      <td>0.1491</td>\n",
       "      <td>0.1683</td>\n",
       "      <td>0.2726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.25</td>\n",
       "      <td>pow</td>\n",
       "      <td>0.5776</td>\n",
       "      <td>0.2693</td>\n",
       "      <td>0.2008</td>\n",
       "      <td>0.2735</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.25</td>\n",
       "      <td>pow</td>\n",
       "      <td>0.5599</td>\n",
       "      <td>0.2757</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.2994</td>\n",
       "      <td>0.2568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.25</td>\n",
       "      <td>pow</td>\n",
       "      <td>0.5638</td>\n",
       "      <td>0.2786</td>\n",
       "      <td>0.1952</td>\n",
       "      <td>0.3082</td>\n",
       "      <td>0.2644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.25</td>\n",
       "      <td>pow</td>\n",
       "      <td>0.5663</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.1615</td>\n",
       "      <td>0.2896</td>\n",
       "      <td>0.2928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.25</td>\n",
       "      <td>pow</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.2691</td>\n",
       "      <td>0.1991</td>\n",
       "      <td>0.2836</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.5</td>\n",
       "      <td>pow</td>\n",
       "      <td>0.5275</td>\n",
       "      <td>0.3073</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>0.2949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.5</td>\n",
       "      <td>pow</td>\n",
       "      <td>0.5359</td>\n",
       "      <td>0.3118</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.1677</td>\n",
       "      <td>0.2678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.5</td>\n",
       "      <td>pow</td>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.3235</td>\n",
       "      <td>0.1451</td>\n",
       "      <td>0.1893</td>\n",
       "      <td>0.2548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.5</td>\n",
       "      <td>pow</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.2951</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.3245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.5</td>\n",
       "      <td>pow</td>\n",
       "      <td>0.5301</td>\n",
       "      <td>0.3102</td>\n",
       "      <td>0.1245</td>\n",
       "      <td>0.1604</td>\n",
       "      <td>0.3378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dropout link     acc     std   ece_h   eve_d   ece_a\n",
       "0    controlled  exp  0.4537  1.6427  0.0318  0.2337  0.2874\n",
       "1   controlled   exp  0.4466  1.3414  0.1081  0.1229  0.2539\n",
       "2    controlled  exp  0.4634  1.4758  0.1013  0.2964  0.1519\n",
       "3    controlled  exp  0.4738  2.8651  0.0913  0.1444  0.1732\n",
       "4    controlled  exp  0.4414  1.0447  0.0864   0.198  0.2725\n",
       "5   controlled   pow  0.4913  2.9921  0.0437  0.3652  0.1345\n",
       "6   controlled   pow  0.4828  1.2842  0.0746  0.0629  0.2415\n",
       "7   controlled   pow  0.4427  3.6516  0.0488  0.2783  0.2793\n",
       "8    controlled  pow  0.3793  1.6634   0.207  0.3545  0.2503\n",
       "9    controlled  pow  0.5023  1.5647   0.016  0.2792   0.108\n",
       "10         0.25  exp  0.5631  0.2587  0.1686  0.2988  0.2766\n",
       "11         0.25  exp  0.5663  0.2719  0.1967  0.2992  0.2638\n",
       "12         0.25  exp  0.5566  0.2611  0.2018  0.3051  0.2708\n",
       "13         0.25  exp  0.5625  0.2659  0.1885  0.3058  0.2676\n",
       "14         0.25  exp  0.5625  0.2633  0.2045  0.2822  0.2765\n",
       "15          0.5  exp  0.5346  0.3057  0.1805  0.1481  0.2765\n",
       "16          0.5  exp  0.5113   0.337  0.1177  0.1483   0.351\n",
       "17          0.5  exp  0.5301  0.3042  0.1469  0.1532  0.3412\n",
       "18          0.5  exp  0.5417  0.3036  0.1152  0.1681  0.3173\n",
       "19          0.5  exp  0.5359  0.3133  0.1491  0.1683  0.2726\n",
       "20         0.25  pow  0.5776  0.2693  0.2008  0.2735  0.2466\n",
       "21         0.25  pow  0.5599  0.2757   0.206  0.2994  0.2568\n",
       "22         0.25  pow  0.5638  0.2786  0.1952  0.3082  0.2644\n",
       "23         0.25  pow  0.5663  0.2576  0.1615  0.2896  0.2928\n",
       "24         0.25  pow   0.565  0.2691  0.1991  0.2836  0.2601\n",
       "25          0.5  pow  0.5275  0.3073  0.1794  0.1429  0.2949\n",
       "26          0.5  pow  0.5359  0.3118  0.1625  0.1677  0.2678\n",
       "27          0.5  pow  0.5385  0.3235  0.1451  0.1893  0.2548\n",
       "28          0.5  pow   0.532  0.2951   0.128   0.162  0.3245\n",
       "29          0.5  pow  0.5301  0.3102  0.1245  0.1604  0.3378"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auswertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = {\"dropout\": [], \"func\": [], \"acc\": [], \"std\": [], \"ece_h\": [], \"ece_d\": [], \"ece_a\": [], \"ece_sum\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_name = \"0.5\"\n",
    "dropout_func = \"exp\"\n",
    "new_df[\"dropout\"].append(dropout_name)\n",
    "new_df[\"func\"].append(dropout_func)\n",
    "a = auswertung[auswertung[\"Dropout\"] == dropout_name]\n",
    "a = a[a[\"link\"] == dropout_func]\n",
    "new_df[\"acc\"].append(a[\"acc\"].astype(float).mean())\n",
    "new_df[\"std\"].append(a[\"std\"].astype(float).mean())\n",
    "new_df[\"ece_h\"].append(a[\"ece_h\"].astype(float).mean())\n",
    "new_df[\"ece_d\"].append(a[\"eve_d\"].astype(float).mean())\n",
    "new_df[\"ece_a\"].append(a[\"ece_a\"].astype(float).mean())\n",
    "new_df[\"ece_sum\"].append(a[\"ece_h\"].astype(float).mean() + a[\"eve_d\"].astype(float).mean() + a[\"ece_a\"].astype(float).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dropout</th>\n",
       "      <th>func</th>\n",
       "      <th>acc</th>\n",
       "      <th>std</th>\n",
       "      <th>ece_h</th>\n",
       "      <th>ece_d</th>\n",
       "      <th>ece_a</th>\n",
       "      <th>ece_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>exp</td>\n",
       "      <td>0.53072</td>\n",
       "      <td>0.31276</td>\n",
       "      <td>0.14188</td>\n",
       "      <td>0.1572</td>\n",
       "      <td>0.31172</td>\n",
       "      <td>0.6108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dropout func      acc      std    ece_h   ece_d    ece_a  ece_sum\n",
       "0     0.5  exp  0.53072  0.31276  0.14188  0.1572  0.31172   0.6108"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Size / Dropout Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    sizes = [4, 8, 16, 32, 128]\n",
    "    dropout = [0, 0.1, 0.25, 0.5, 0.8]\n",
    "    # expo = [True, False]\n",
    "\n",
    "    df_s, df_d, df_e, df_std_h, df_std_a = [[],[],[],[],[]], [[],[],[],[],[]], [[],[],[],[],[]], [[],[],[],[],[]], [[],[],[],[],[]]\n",
    "    df_train_loss, df_test_loss, df_train_acc, df_test_acc = [[],[],[],[],[]], [[],[],[],[],[]], [[],[],[],[],[]], [[],[],[],[],[]]\n",
    "    # for s, d, e in itertools.product(sizes, dropout, expo):\n",
    "    for i in range(1):\n",
    "        for s, d in itertools.product(sizes, dropout):\n",
    "            model = None\n",
    "            df_s[i].append(s)\n",
    "            df_d[i].append(d)\n",
    "            # df_e.append(e)\n",
    "            model = Sequential()\n",
    "            model.add(InputLayer(14))\n",
    "            model.add(Dropout(d))\n",
    "            model.add(Dense(s, activation=\"relu\"))\n",
    "            # model.add(ControlledDropoutLayer(16, dch))\n",
    "            model.add(Dense(2, activation=\"linear\"))\n",
    "            # if e:\n",
    "            #     model.add(ExponentialLayer(2))\n",
    "\n",
    "            # compile the keras model\n",
    "            model.compile(loss=my_loss, optimizer=\"adam\", metrics=my_acc)\n",
    "\n",
    "            history = model.fit(X, y, epochs=50, batch_size=10)\n",
    "\n",
    "            eval_loss, eval_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "            df_train_loss[i].append(history.history[\"loss\"][-1])\n",
    "            df_train_acc[i].append(history.history[\"my_acc\"][-1])\n",
    "            df_test_loss[i].append(eval_loss)\n",
    "            df_test_acc[i].append(eval_acc)\n",
    "\n",
    "            preds = []\n",
    "            for x in range(1000):\n",
    "                preds.append(model(X_test, training=True))\n",
    "\n",
    "            preds = np.stack(preds)\n",
    "            # # preds = preds.astype(int)\n",
    "\n",
    "            predictions_home = np.swapaxes(preds, 0, 1)[:, :, 0]\n",
    "            predictions_away = np.swapaxes(preds, 0, 1)[:, :, 1]\n",
    "\n",
    "            home_std = np.std(predictions_home, axis=1)\n",
    "            away_std = np.std(predictions_away, axis=1)\n",
    "\n",
    "            df_std_h[i].append(home_std.mean())\n",
    "            df_std_a[i].append(away_std.mean())\n",
    "if True: # plot dropout std analysis\n",
    "    auswertung = pd.read_csv(\"mlp_dropout_std.csv\", sep=\";\")\n",
    "    # plot auswertung\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.plot(auswertung[auswertung[\"param_size\"] == 4][\"dropout\"], auswertung[auswertung[\"param_size\"] == 4][\"std\"], color=\"r\", label=\"4\")\n",
    "    ax.plot(auswertung[auswertung[\"param_size\"] == 8][\"dropout\"], auswertung[auswertung[\"param_size\"] == 8][\"std\"], color=\"b\", label=\"8\")\n",
    "    ax.plot(auswertung[auswertung[\"param_size\"] == 16][\"dropout\"], auswertung[auswertung[\"param_size\"] == 16][\"std\"], color=\"g\", label=\"16\")\n",
    "    ax.plot(auswertung[auswertung[\"param_size\"] == 32][\"dropout\"], auswertung[auswertung[\"param_size\"] == 32][\"std\"], color=\"y\", label=\"32\")\n",
    "    ax.plot(auswertung[auswertung[\"param_size\"] == 128][\"dropout\"], auswertung[auswertung[\"param_size\"] == 128][\"std\"], color=\"k\", label=\"128\")\n",
    "    ax.set_ylabel(\"Std of goals predicted\")\n",
    "    ax.set_xlabel(\"Dropout rate\")\n",
    "    ax.legend(loc=4, title=\"Hidden layer size\")\n",
    "if False: # plot mlp size analysis\n",
    "    fig, axs = plt.subplots(1,3, figsize=(15, 5))\n",
    "    axs[0].plot(auswertung[auswertung[\"dropout\"] == 0][[\"param_size\"]], auswertung[auswertung[\"dropout\"] == 0][[\"train_loss\"]], \"r-\", label='Train Loss')\n",
    "    axs[0].plot(auswertung[auswertung[\"dropout\"] == 0][[\"param_size\"]], auswertung[auswertung[\"dropout\"] == 0][[\"test_loss\"]], \"r--\", label=\"Test Loss\")\n",
    "    ax0_twin = axs[0].twinx()\n",
    "    ax0_twin.plot(auswertung[auswertung[\"dropout\"] == 0][[\"param_size\"]], auswertung[auswertung[\"dropout\"] == 0][[\"train_acc\"]], \"k-\", label='Train Accuracy')\n",
    "    ax0_twin.plot(auswertung[auswertung[\"dropout\"] == 0][[\"param_size\"]], auswertung[auswertung[\"dropout\"] == 0][[\"test_acc\"]], \"k--\", label=\"Test Accuracy\")\n",
    "    axs[0].set_xticks(auswertung[auswertung[\"dropout\"] == 0][[\"param_size\"]].values)\n",
    "    axs[0].set_ylim([1,1.7])\n",
    "    ax0_twin.set_ylim([0.5,0.65])\n",
    "    axs[0].legend(loc=2)\n",
    "    # ax0_twin.legend(loc=1)\n",
    "    axs[1].plot(auswertung[auswertung[\"dropout\"] == 0.25][[\"param_size\"]], auswertung[auswertung[\"dropout\"] == 0.25][[\"train_loss\"]], \"r-\", label='Train Loss')\n",
    "    axs[1].plot(auswertung[auswertung[\"dropout\"] == 0.25][[\"param_size\"]], auswertung[auswertung[\"dropout\"] == 0.25][[\"test_loss\"]], \"r--\", label=\"Test Loss\")\n",
    "    ax1_twin = axs[1].twinx()\n",
    "    ax1_twin.plot(auswertung[auswertung[\"dropout\"] == 0.25][[\"param_size\"]], auswertung[auswertung[\"dropout\"] == 0.25][[\"train_acc\"]], \"k-\", label='Train Accuracy')\n",
    "    ax1_twin.plot(auswertung[auswertung[\"dropout\"] == 0.25][[\"param_size\"]], auswertung[auswertung[\"dropout\"] == 0.25][[\"test_acc\"]], \"k--\", label=\"Test Accuracy\")\n",
    "    axs[1].set_xticks(auswertung[auswertung[\"dropout\"] == 0.25][[\"param_size\"]].values)\n",
    "    axs[1].set_ylim([1,1.7])\n",
    "    ax1_twin.set_ylim([0.5,0.65])\n",
    "    #axs[1].legend(loc=2)\n",
    "    #ax1_twin.legend(loc=1)\n",
    "    axs[2].plot(auswertung[auswertung[\"dropout\"] == 0.5][[\"param_size\"]], auswertung[auswertung[\"dropout\"] == 0.5][[\"train_loss\"]], \"r-\", label='Train Loss')\n",
    "    axs[2].plot(auswertung[auswertung[\"dropout\"] == 0.5][[\"param_size\"]], auswertung[auswertung[\"dropout\"] == 0.5][[\"test_loss\"]], \"r--\", label=\"Test Loss\")\n",
    "    ax2_twin = axs[2].twinx()\n",
    "    ax2_twin.plot(auswertung[auswertung[\"dropout\"] == 0.5][[\"param_size\"]], auswertung[auswertung[\"dropout\"] == 0.5][[\"train_acc\"]], \"k-\", label='Train Accuracy')\n",
    "    ax2_twin.plot(auswertung[auswertung[\"dropout\"] == 0.5][[\"param_size\"]], auswertung[auswertung[\"dropout\"] == 0.5][[\"test_acc\"]], \"k--\", label=\"Test Accuracy\")\n",
    "    axs[2].set_xticks(auswertung[auswertung[\"dropout\"] == 0.5][[\"param_size\"]].values)\n",
    "    axs[2].set_ylim([1,1.7])\n",
    "    ax2_twin.set_ylim([0.5,0.65])\n",
    "    # axs[2].legend(loc=2)\n",
    "    ax2_twin.legend(loc=1)\n",
    "    axs[0].set_xscale(\"log\")\n",
    "    axs[1].set_xscale(\"log\")\n",
    "    axs[2].set_xscale(\"log\")\n",
    "    axs[0].set_xticks([4,8,16,32,128], [\"4\",\"8\",\"16\",\"32\",\"128\"])\n",
    "    axs[1].set_xticks([4,8,16,32,128], [\"4\",\"8\",\"16\",\"32\",\"128\"])\n",
    "    axs[2].set_xticks([4,8,16,32,128], [\"4\",\"8\",\"16\",\"32\",\"128\"])\n",
    "    axs[1].set_yticks(np.arange(1,1.7,0.1),[])\n",
    "    ax1_twin.set_yticks(np.arange(0.5,0.65,0.02),[])\n",
    "    ax0_twin.set_yticks(np.arange(0.5,0.65,0.02),[])\n",
    "    axs[2].set_yticks(np.arange(1,1.7,0.1),[])\n",
    "    axs[0].set_title(\"Dropout=0.00\")\n",
    "    axs[1].set_title(\"Dropout=0.25\")\n",
    "    axs[2].set_title(\"Dropout=0.50\")\n",
    "    axs[0].set_yticks(np.arange(1,1.71,0.1),np.around(np.arange(1,1.71,0.1),2).astype(str),color=\"r\")\n",
    "    axs[0].set_ylabel(\"Loss\", color=\"r\")\n",
    "    ax2_twin.set_ylabel(\"Accuracy\")\n",
    "    axs[0].set_xlabel(\"Hidden Layer Size\")\n",
    "    axs[1].set_xlabel(\"Hidden Layer Size\")\n",
    "    axs[2].set_xlabel(\"Hidden Layer Size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlled Dropout Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # run cell or not\n",
    "    run_params = []\n",
    "    run_params.append(dropout_conf_1())\n",
    "    run_params.append(dropout_conf_2())\n",
    "    run_params.append(dropout_conf_3())\n",
    "    dconf1, dconf2 = dropout_conf_1()\n",
    "    tmp1, tmp2 = dropout_conf_2()\n",
    "    dconf1 = np.concatenate([dconf1, tmp1])\n",
    "    dconf2 = np.concatenate([dconf2, tmp2])\n",
    "    run_params.append((dconf1, dconf2))\n",
    "    dconf1, dconf2 = dropout_conf_1()\n",
    "    tmp1, tmp2 = dropout_conf_3()\n",
    "    dconf1 = np.concatenate([dconf1, tmp1])\n",
    "    dconf2 = np.concatenate([dconf2, tmp2])\n",
    "    run_params.append((dconf1, dconf2))\n",
    "    dconf1, dconf2 = dropout_conf_2()\n",
    "    tmp1, tmp2 = dropout_conf_3()\n",
    "    dconf1 = np.concatenate([dconf1, tmp1])\n",
    "    dconf2 = np.concatenate([dconf2, tmp2])\n",
    "    run_params.append((dconf1, dconf2))\n",
    "    dconf1, dconf2 = dropout_conf_1()\n",
    "    tmp1, tmp2 = dropout_conf_2()\n",
    "    tmp3, tmp4 = dropout_conf_3()\n",
    "    dconf1 = np.concatenate([dconf1, tmp1, tmp3])\n",
    "    dconf2 = np.concatenate([dconf2, tmp2, tmp4])\n",
    "    run_params.append((dconf1, dconf2))\n",
    "\n",
    "    dropout_version_df, test_acc_df, std_df, ece_h_df, ece_d_df, ece_a_df = [[],[],[],[],[]], [[],[],[],[],[]], [[],[],[],[],[]], [[],[],[],[],[]], [[],[],[],[],[]], [[],[],[],[],[]]\n",
    "\n",
    "    for i in range(5):\n",
    "        for d_idx, (dconf1, dconf2) in enumerate(run_params):\n",
    "            step_size = len(dconf1)\n",
    "            dropout_version_df[i].append(d_idx)\n",
    "            model = Sequential()\n",
    "            model.add(InputLayer(14))\n",
    "            model.add(ControlledDropoutLayer(dconf1))\n",
    "            model.add(Dense(16, activation=\"relu\"))\n",
    "            model.add(ControlledDropoutLayer(dconf2))\n",
    "            model.add(Dense(2, activation=\"linear\"))\n",
    "            # model.add(ExponentialLayer(2))\n",
    "\n",
    "            model.compile(loss=my_loss, optimizer='adam', metrics=my_acc)\n",
    "            hist = model.fit(X, y, epochs=100, steps_per_epoch=step_size, shuffle=True)\n",
    "\n",
    "            preds = []\n",
    "            for x in range(step_size):\n",
    "                preds.append(model(X_test))\n",
    "\n",
    "            preds = np.stack(preds)\n",
    "            predictions_home = np.swapaxes(preds, 0, 1)[:, :, 0]\n",
    "            predictions_away = np.swapaxes(preds, 0, 1)[:, :, 1]\n",
    "            home_std = np.std(predictions_home, axis=1)\n",
    "            away_std = np.std(predictions_away, axis=1)\n",
    "\n",
    "            std_df[i].append(((home_std.mean() + away_std.mean()) / 2))\n",
    "\n",
    "            act_res = []\n",
    "            for h, a in y_test:\n",
    "                act_res.append(f\"{str(int(h))}:{str(int(a))}\")\n",
    "\n",
    "            predictions_home = np.swapaxes(preds, 0, 1)[:, :, 0]\n",
    "            predictions_away = np.swapaxes(preds, 0, 1)[:, :, 1]\n",
    "\n",
    "            game_quotes = []\n",
    "            most_goals = {\"home\": [], \"away\": []}\n",
    "            for game_idx in range(len(predictions_home)):\n",
    "                game_df = pd.DataFrame(\n",
    "                    {\"home\": predictions_home[game_idx], \"away\": predictions_away[game_idx]}\n",
    "                )\n",
    "                game_df[\"diff\"] = game_df[\"home\"] - game_df[\"away\"]\n",
    "                game_df[\"clipped_res\"] = np.clip(game_df[\"diff\"], -1, 1)\n",
    "                game_df[\"rounded_res\"] = np.rint(game_df[\"clipped_res\"])\n",
    "                home = game_df.loc[game_df[\"rounded_res\"] == 1].shape[0] / step_size\n",
    "                draw = (\n",
    "                    game_df.loc[game_df[\"rounded_res\"] == 0].shape[0] / step_size\n",
    "                )\n",
    "                away = game_df.loc[game_df[\"rounded_res\"] == -1].shape[0] / step_size\n",
    "\n",
    "                game_quotes.append(f\"{round(home, 3)}-{round(draw, 3)}-{round(away, 3)}\")\n",
    "\n",
    "            df_res = pd.DataFrame(\n",
    "                {\"actual\": act_res, \"predicted\": game_quotes}\n",
    "            ) \n",
    "            df_cross = pd.DataFrame(\n",
    "                {\n",
    "                    \"actual\": [\n",
    "                        0\n",
    "                        if int(df_res.iloc[i][\"actual\"].split(\":\")[0])\n",
    "                        > int(df_res.iloc[i][\"actual\"].split(\":\")[1])\n",
    "                        else 1\n",
    "                        if int(df_res.iloc[i][\"actual\"].split(\":\")[0])\n",
    "                        == int(df_res.iloc[i][\"actual\"].split(\":\")[1])\n",
    "                        else 2\n",
    "                        for i in range(df_res.shape[0])\n",
    "                    ],\n",
    "                    \"pred\": [\n",
    "                        np.argmax([float(y) for y in df_res.iloc[i][\"predicted\"].split(\"-\")])\n",
    "                        for i in range(df_res.shape[0])\n",
    "                    ],\n",
    "                    \"pred_val\": [\n",
    "                        np.max([float(y) for y in df_res.iloc[i][\"predicted\"].split(\"-\")])\n",
    "                        for i in range(df_res.shape[0])\n",
    "                    ],\n",
    "                    \"bookie\": [\n",
    "                        np.argmax(\n",
    "                            [\n",
    "                                test_data.iloc[i].bookie_home,\n",
    "                                test_data.iloc[i].bookie_draw,\n",
    "                                test_data.iloc[i].bookie_away,\n",
    "                            ]\n",
    "                        )\n",
    "                        for i in range(test_data.shape[0])\n",
    "                    ],\n",
    "                    \"bookie_val\": [\n",
    "                        np.max(\n",
    "                            [\n",
    "                                test_data.iloc[i].bookie_home,\n",
    "                                test_data.iloc[i].bookie_draw,\n",
    "                                test_data.iloc[i].bookie_away,\n",
    "                            ]\n",
    "                        )\n",
    "                        for i in range(test_data.shape[0])\n",
    "                    ],\n",
    "                }\n",
    "            )\n",
    "            right, wrong, home_right, draw_right, away_right = 0, 0, 0, 0, 0\n",
    "            home_pred = {\"act_away\": 0, \"act_draw\": 0}\n",
    "            draw_pred = {\"act_away\": 0, \"act_home\": 0}\n",
    "            away_pred = {\"act_home\": 0, \"act_draw\": 0}\n",
    "            for x in range(df_cross.shape[0]):\n",
    "                if df_cross.iloc[x][\"actual\"] != df_cross.iloc[x][\"pred\"]:\n",
    "                    if df_cross.iloc[x][\"pred\"] == 0:\n",
    "                        if df_cross.iloc[x][\"actual\"] == 1:\n",
    "                            home_pred[\"act_draw\"] += 1\n",
    "                        else:\n",
    "                            home_pred[\"act_away\"] += 1\n",
    "                    elif df_cross.iloc[x][\"pred\"] == 1:\n",
    "                        if df_cross.iloc[x][\"actual\"] == 0:\n",
    "                            draw_pred[\"act_home\"] += 1\n",
    "                        else:\n",
    "                            draw_pred[\"act_away\"] += 1\n",
    "                    else:\n",
    "                        if df_cross.iloc[x][\"actual\"] == 0:\n",
    "                            away_pred[\"act_home\"] += 1\n",
    "                        else:\n",
    "                            away_pred[\"act_draw\"] += 1\n",
    "                    wrong += 1\n",
    "                else:\n",
    "                    if df_cross.iloc[x][\"actual\"] == 0:\n",
    "                        home_right += 1\n",
    "                    elif df_cross.iloc[x][\"actual\"] == 1:\n",
    "                        draw_right += 1\n",
    "                    else:\n",
    "                        away_right += 1\n",
    "                    right += 1\n",
    "\n",
    "            home_wrong = home_pred[\"act_away\"] + home_pred[\"act_draw\"]\n",
    "            draw_wrong = draw_pred[\"act_away\"] + draw_pred[\"act_home\"]\n",
    "            away_wrong = away_pred[\"act_home\"] + away_pred[\"act_draw\"]\n",
    "            acc = right / df_cross.shape[0]\n",
    "            test_acc_df[i].append(acc)\n",
    "            \n",
    "            # ece\n",
    "            data_length = df_cross.shape[0]\n",
    "            acc_home = (\n",
    "                df_cross.loc[(df_cross[\"pred\"] == 0) & (df_cross[\"actual\"] == 0)].shape[0]\n",
    "            ) / df_cross.loc[df_cross[\"actual\"] == 0].shape[0]\n",
    "            acc_draw = (\n",
    "                df_cross.loc[(df_cross[\"pred\"] == 1) & (df_cross[\"actual\"] == 1)].shape[0]\n",
    "            ) / df_cross.loc[df_cross[\"actual\"] == 1].shape[0]\n",
    "            acc_away = (\n",
    "                df_cross.loc[(df_cross[\"pred\"] == 2) & (df_cross[\"actual\"] == 2)].shape[0]\n",
    "            ) / df_cross.loc[df_cross[\"actual\"] == 2].shape[0]\n",
    "\n",
    "            conf_home = np.mean(df_cross.loc[(df_cross[\"pred\"] == 0)].pred_val)\n",
    "            conf_draw = np.mean(df_cross.loc[(df_cross[\"pred\"] == 1)].pred_val)\n",
    "            conf_away = np.mean(df_cross.loc[(df_cross[\"pred\"] == 2)].pred_val)\n",
    "            ece_h_df[i].append(abs(acc_home - conf_home))\n",
    "            ece_d_df[i].append(abs(acc_draw - conf_draw))\n",
    "            ece_a_df[i].append(abs(acc_away - conf_away))\n",
    "\n",
    "    auswertung = pd.DataFrame({\"dropout_conf\": np.mean(dropout_version_df, axis=0), \"test_acc\": np.mean(test_acc_df, axis=0), \"std\": np.mean(std_df, axis=0), \"ece_h\": np.mean(ece_h_df, axis=0), \"ece_d\": np.mean(ece_d_df, axis=0), \"ece_a\": np.mean(ece_a_df, axis=0)})\n",
    "    auswertung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    weights_list = []\n",
    "    for _ in range(25):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(16, input_shape=(14,), activation=\"relu\"))\n",
    "        # model.add(Dropout(0.5))\n",
    "        model.add(Dense(2, activation=\"linear\"))\n",
    "        model.add(ExponentialLayer(2))\n",
    "\n",
    "        model.compile(loss=my_loss, optimizer='adam', metrics=my_acc)\n",
    "        hist = model.fit(X, y, epochs=25, batch_size=10)\n",
    "        weights_list.append(abs(model.weights[0]))\n",
    "    # vizualize the results\n",
    "    swl = np.mean(weights_list, axis=0)\n",
    "    swl = swl[:13]\n",
    "    ax = sb.heatmap(abs(swl), cmap=\"Blues\")\n",
    "    ax.set_yticks(np.arange(0.5, 13.5, 1), labels=['Home xG',\n",
    "                                                \"Away xG\", \n",
    "                                                \"Home $\\overline{xG}$\",\n",
    "                                                \"Away $\\overline{xG}$\",\n",
    "                                                \"Home xT\",\n",
    "                                                \"Away xT\",\n",
    "                                                \"Home $\\overline{xT}$\",\n",
    "                                                \"Away $\\overline{xT}$\",\n",
    "                                                \"Form Home\",\n",
    "                                                \"Form Away\",\n",
    "                                                \"$\\overline{Form Home}$\",\n",
    "                                                \"$\\overline{Form Away}$\",\n",
    "                                                \"ELO Diff\"],\n",
    "                                                    rotation=0)\n",
    "    ax.set_ylabel(\"Feature\")\n",
    "    ax.set_xlabel(\"Neuron\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "packing-report",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "712082fa5832080cfbc451c8f4e95f25cd39c626d7d8aebf8310513402cfb031"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
